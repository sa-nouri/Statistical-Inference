{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "options(repr.plot.width=5, repr.plot.height=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Diagnostics in multiple linear regression\n",
    "\n",
    "### Outline\n",
    "\n",
    "-   Diagnostics – again\n",
    "\n",
    "-   Different types of residuals\n",
    "\n",
    "-   Influence\n",
    "\n",
    "-   Outlier detection\n",
    "\n",
    "-   Residual plots:\n",
    "\n",
    "    -   partial regression (added variable) plot,\n",
    "\n",
    "    -   partial residual (residual plus component) plot.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Scottish hill races data\n",
    "\n",
    "The dataset we will use is based on record times on [Scottish hill races](http://www.statsci.org/data/general/hills.html).\n",
    "\n",
    "<table>\n",
    "<tr><td><b>Variable</b></td><td><b>Description</b></td></tr>\n",
    "<tr><td>Time</td><td>Record time to complete course</td></tr>\n",
    "<tr><td>Distance</td><td>Distance in the course</td></tr>\n",
    "<tr><td>Climb</td><td>Vertical climb in the course</td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "url = 'http://www.statsci.org/data/general/hills.txt' \n",
    "races.table = read.table(url, header=TRUE, sep='\\t')\n",
    "head(races.table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "As we'd expect, the time increases both with `Distance` and `Climb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plot(races.table[,2:4], pch=23, bg='orange', cex=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's look at our multiple regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "races.lm = lm(Time ~ Distance + Climb, data=races.table)\n",
    "summary(races.lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "But is this a good model? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Diagnostics\n",
    "\n",
    "### What can go wrong?\n",
    "\n",
    "-   Regression function can be wrong: maybe regression function should\n",
    "    have some other form (see diagnostics for simple linear regression).\n",
    "\n",
    "-   Model for the errors may be incorrect:\n",
    "\n",
    "    -   may not be normally distributed.\n",
    "\n",
    "    -   may not be independent.\n",
    "\n",
    "    -   may not have the same variance.\n",
    "\n",
    "-   Detecting problems is more *art* then *science*, i.e. we cannot\n",
    "    *test* for all possible problems in a regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Diagnostics\n",
    "\n",
    "-   Basic idea of diagnostic measures: if model is correct then\n",
    "    residuals $e_i = Y_i -\\widehat{Y}_i, 1 \\leq i \\leq n$ should look\n",
    "    like a sample of (not quite independent) $N(0, \\sigma^2)$ random\n",
    "    variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Standard diagnostic plots\n",
    "\n",
    "`R` produces a set of standard plots for `lm` that help us assess whether our assumptions are reasonable or not. We will go through each in some, but not too much, detail.\n",
    "\n",
    "As we see below, there are some quantities which we need to define in order to read these plots. We will define these first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "par(mfrow=c(2,2))\n",
    "plot(races.lm, pch=23 ,bg='orange',cex=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Problems with the errors\n",
    "\n",
    "### Possible problems & diagnostic checks\n",
    "\n",
    "-   Errors may not be normally distributed or may not have the same\n",
    "    variance – qqnorm can help with this. This may not be too important\n",
    "    in large samples.\n",
    "\n",
    "-   Variance may not be constant. Can also be addressed in a plot of $X$ vs. $e$\n",
    ": *fan shape* or other trend indicate\n",
    "    non-constant variance.\n",
    "\n",
    "-   Influential observations. Which points “affect” the regression line\n",
    "    the most?\n",
    "\n",
    "-   Outliers: points where the model really does not fit! Possibly\n",
    "    mistakes in data transcription, lab errors, who knows? Should be\n",
    "    recognized and (hopefully) explained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Types of residuals\n",
    "\n",
    "-   Ordinary residuals: $e_i = Y_i - \\widehat{Y}_i$. These measure the\n",
    "    deviation of predicted value from observed value, but their\n",
    "    distribution depends on unknown scale, $\\sigma$.\n",
    "\n",
    "-   Internally studentized residuals (`rstandard` in R):\n",
    "    $$r_i = e_i / SE(e_i) = \\frac{e_i}{\\widehat{\\sigma} \\sqrt{1 - H_{ii}}}$$\n",
    "    \n",
    "- Above, $H$ is the “hat” matrix $H=X(X^TX)^{-1}X^T$. These are almost $t$-distributed, except\n",
    "    $\\widehat{\\sigma}$ depends on $e_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Types of residuals\n",
    "\n",
    "- Externally studentized residuals (`rstudent` in R):\n",
    "    $$t_i = \\frac{e_i}{\\widehat{\\sigma_{(i)}} \\sqrt{1 - H_{ii}}} \\sim t_{n-p-2}.$$\n",
    "    These are exactly $t$ distributed so we know their distribution and\n",
    "    can use them for tests, if desired.\n",
    "    \n",
    "- The quantity $\\hat{\\sigma}^2_{(i)}$ is the MSE of the model fit to all data except case $i$ (i.e. it has $n-1$ observations and $p$ features).\n",
    "\n",
    "- Numerically, these residuals are highly correlated, as we would expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plot(resid(races.lm), rstudent(races.lm), pch=23, bg='blue', cex=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plot(rstandard(races.lm), rstudent(races.lm), pch=23, bg='blue', cex=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Standard diagnostic plots\n",
    "\n",
    "The first plot is the quantile plot for the residuals, that compares their distribution\n",
    "to that of a sample of independent normals.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "qqnorm(rstandard(races.lm), pch=23, bg='red', cex=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If the residuals were really normal we'd expect this plot to be roughly on the diagonal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "qqnorm(rnorm(500), pch=23, bg='red', cex=2)\n",
    "abline(0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Two other plots try address the constant variance assumptions. If these plots\n",
    "have a particular shape (maybe the spread increases with $\\hat{Y}$) then maybe the variance is not constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plot(fitted(races.lm), sqrt(abs(rstandard(races.lm))), pch=23, bg='red', ylim=c(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plot(fitted(races.lm), resid(races.lm), pch=23, bg='red', cex=2)\n",
    "abline(h=0, lty=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Influence of an observation\n",
    "\n",
    "Other plots provide an assessment of the `influence` of each observation.\n",
    "Usually, this is done by dropping an entire case $(y_i, x_i)$ from the dataset and\n",
    "refitting the model.\n",
    "\n",
    "-   In this setting, a $\\cdot_{(i)}$ indicates $i$-th observation was\n",
    "    not used in fitting the model.\n",
    "\n",
    "-   For example: $\\widehat{Y}_{j(i)}$ is the regression function\n",
    "    evaluated at the $j$-th observation predictors BUT the coefficients\n",
    "    $(\\widehat{\\beta}_{0(i)}, \\dots, \\widehat{\\beta}_{p(i)})$ were fit\n",
    "    after deleting $i$-th case from the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Influence of an observation\n",
    "\n",
    "-   Idea: if $\\widehat{Y}_{j(i)}$ is very different than $\\widehat{Y}_j$\n",
    "    (using all the data) then $i$ is an influential point, at least for\n",
    "    estimating the regression function at $(X_{1,j}, \\dots, X_{p,j})$.\n",
    "    \n",
    "- Could also look at difference between $\\widehat{Y}_{i(i)} - \\widehat{Y}_i$, or any other measure.\n",
    "    \n",
    "-  There are various standard measures of influence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## DFFITS\n",
    "\n",
    "-   $$DFFITS_i = \\frac{\\widehat{Y}_i - \\widehat{Y}_{i(i)}}{\\widehat{\\sigma}_{(i)} \\sqrt{H_{ii}}}$$\n",
    "\n",
    "-   This quantity measures how much the regression function changes at\n",
    "    the $i$-th case / observation when the $i$-th case / observation is\n",
    "    deleted.\n",
    "\n",
    "-   For small/medium datasets: value of 1 or greater is “suspicious” (RABE).\n",
    "    For large dataset: value of $2 \\sqrt{(p+1)/n}$.\n",
    "    \n",
    "- `R` has its own standard rules similar to the above for marking an observation\n",
    "as influential.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plot(dffits(races.lm), pch=23, bg='orange', cex=2, ylab=\"DFFITS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "It seems that some observations had a high influence measured by $DFFITS$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "races.table[which(dffits(races.lm) > 0.5),]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "It is perhaps not surprising that the longest course and the course with the most elevation gain seemed to have a strong effect on the fitted values. What about `Knock Hill`? We'll come back to this later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cook’s distance\n",
    "\n",
    "Cook’s distance measures how much the entire regression function\n",
    "    changes when the $i$-th case is deleted.\n",
    "\n",
    "-   $$D_i = \\frac{\\sum_{j=1}^n(\\widehat{Y}_j - \\widehat{Y}_{j(i)})^2}{(p+1) \\, \\widehat{\\sigma}^2}$$\n",
    "\n",
    "-   Should be comparable to $F_{p+1,n-p-1}$: if the “$p$-value” of $D_i$\n",
    "    is 50 percent or more, then the $i$-th case is likely influential:\n",
    "    investigate further. (RABE)\n",
    "    \n",
    "- Again, `R` has its own rules similar to the above for marking an observation\n",
    "as influential.\n",
    "\n",
    "- What to do after investigation? No easy answer.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plot(cooks.distance(races.lm), pch=23, bg='orange', cex=2, ylab=\"Cook's distance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "races.table[which(cooks.distance(races.lm) > 0.1),]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Again, the same 3 races. This is not surprising as both $DFFITS$ and Cook's distance measure changes in fitted values. The difference is that one measures the influence on one fitted value, while the other measures the influence on the entire vector of fitted values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## DFBETAS\n",
    "\n",
    "This quantity measures how much the coefficients change when the\n",
    "    $i$-th case is deleted.\n",
    "\n",
    "\n",
    "-   $$DFBETAS_{j(i)} = \\frac{\\widehat{\\beta}_j - \\widehat{\\beta}_{j(i)}}{\\sqrt{\\widehat{\\sigma}^2_{(i)} (X^TX)^{-1}_{jj}}}.$$\n",
    "\n",
    "   \n",
    "-   For small/medium datasets: absolute value of 1 or greater is\n",
    "    “suspicious”. For large dataset: absolute value of $2 /  \\sqrt{n}$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plot(dfbetas(races.lm)[,'Climb'], pch=23, bg='orange', cex=2, ylab=\"DFBETA (Climb)\")\n",
    "races.table[which(abs(dfbetas(races.lm)[,'Climb']) > 1),]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plot(dfbetas(races.lm)[,'Distance'], pch=23, bg='orange', cex=2, ylab=\"DFBETA (Climb)\")\n",
    "races.table[which(abs(dfbetas(races.lm)[,'Distance']) > 0.5),]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Outliers\n",
    "\n",
    "The essential definition of an *outlier* is an observation pair $(Y, X_1, \\dots, X_p)$ that does not follow the model, while most other observations seem to follow the model.\n",
    "\n",
    "-   Outlier in *predictors*: the $X$ values of the observation may lie\n",
    "    outside the “cloud” of other $X$ values. This means you may be\n",
    "    extrapolating your model inappropriately. The values $H_{ii}$ can be\n",
    "    used to measure how “outlying” the $X$ values are.\n",
    "\n",
    "-   Outlier in *response*: the $Y$ value of the observation may lie very\n",
    "    far from the fitted model. If the studentized residuals are large:\n",
    "    observation may be an outlier.\n",
    "    \n",
    "- The races at `Bens of Jura` and `Lairig Ghru` seem to be outliers in *predictors*\n",
    "as they were the highest and longest races, respectively.\n",
    "\n",
    "- How can we tell if the `Knock Hill` result is an outlier? It seems to have taken much\n",
    "longer than it should have so maybe it is an outlier in the *response*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Outlying $X$ values\n",
    "\n",
    "One way to detect outliers in the *predictors*, besides just looking at the actual values themselves, is through their leverage values, defined by\n",
    "$$\n",
    "\\text{leverage}_i = H_{ii} = (X(X^TX)^{-1}X^T)_{ii}.\n",
    "$$\n",
    "\n",
    "Not surprisingly, our longest and highest courses show up again. This at least\n",
    "reassures us that the leverage is capturing some of this \"outlying in $X$ space\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plot(hatvalues(races.lm), pch=23, bg='orange', cex=2, ylab='Hat values')\n",
    "races.table[which(hatvalues(races.lm) > 0.3),]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Outliers in the response\n",
    "\n",
    "We will consider a crude outlier test that tries to find residuals that are\n",
    "\"larger\" than they should be.\n",
    "\n",
    "- Since `rstudent` are $t$ distributed, we could just compare them to the $T$ distribution and reject if their absolute value is too large.\n",
    "\n",
    "- Doing this for every observation results in $n$ different hypothesis tests.\n",
    "\n",
    "-   This causes a problem: if $n$ is large, if we “threshold” at\n",
    "    $t_{1-\\alpha/2, n-p-2}$ we will get many outliers by chance even if\n",
    "    model is correct. \n",
    "    \n",
    "- In fact, we expect to see $n \\cdot \\alpha$\n",
    "    “outliers” by this test. Every large data set would have outliers in\n",
    "    it, even if model was entirely correct!\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's sample some data from our model to convince ourselves that this is a real problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X = rnorm(100)\n",
    "Y = 2 * X + 0.5 + rnorm(100)\n",
    "alpha = 0.1\n",
    "cutoff = qt(1 - alpha / 2, 97)\n",
    "sum(abs(rstudent(lm(Y~X))) > cutoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Bonferroni correction\n",
    "X = rnorm(100)\n",
    "Y = 2 * X + 0.5 + rnorm(100)\n",
    "cutoff = qt(1 - (alpha / 100) / 2, 97)\n",
    "sum(abs(rstudent(lm(Y~X))) > cutoff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Multiple comparisons\n",
    "\n",
    "-   This problem we identified is known as *multiple comparisons* or *simultaneous inference.* \n",
    "\n",
    "- When performing many tests (say $m$) each at level $\\alpha$, we expect at least $\\alpha m$ rejections\n",
    "even when *all* null hypotheses are true!\n",
    "\n",
    "- In outlier detection, we are performing $m=n$ hypothesis tests, but might still\n",
    "like to control the probability of making *any* false positive\n",
    "errors.\n",
    "    \n",
    "- The reason we don't want to make errors here is that we don't\n",
    "want to throw away data unnecessarily.\n",
    "\n",
    "- One solution: Bonferroni correction, threshold at\n",
    "$t_{1 - \\alpha/(2*n), n-p-2}$.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bonferroni correction\n",
    "\n",
    "- Dividing $\\alpha$ by $n$, the number of tests, is known as a *Bonferroni* correction.\n",
    "\n",
    "-  If we are doing many $t$ (or other) tests, say $m \\gg 1$ we can\n",
    "  control overall false positive rate at $\\alpha$ by testing each one\n",
    "  at level $\\alpha/m$. \n",
    "  \n",
    "- In this case $m=n$, but other times we might look at a different number of tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bonferroni correction\n",
    "\n",
    "- Essentially the *union bound* for probability.\n",
    "\n",
    "- **Proof:** when the model is correct, with studentized residuals $T_i$:\n",
    "\n",
    "    $$\\begin{aligned}\n",
    "        P\\left( \\text{at least one false positive} \\right)\n",
    "        &  = P \\left(\\cup_{i=1}^m |T_i| \\geq t_{1 - \\alpha/(2*m), n-p-2} \\right) \\\\\n",
    "        & \\leq \\sum_{i=1}^m P \\left( |T_i| \\geq t_{1 - \\alpha/(2*m), n-p-2} \\right) \\\\\n",
    "        &  = \\sum_{i=1}^m  \\frac{\\alpha}{m} = \\alpha. \\\\\n",
    "       \\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's apply this to our data. It turns out that `KnockHill` is a [known error](http://www.statsci.org/data/general/hills.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "n = nrow(races.table)\n",
    "cutoff = qt(1 - 0.05 / (2*n), (n - 4))\n",
    "races.table[which(abs(rstudent(races.lm)) > cutoff),]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The package `car` has a built in function to do this test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "library(car)\n",
    "outlierTest(races.lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Final plot\n",
    "\n",
    "The last plot that `R` produces is a plot of residuals against leverage. Points that have\n",
    "high leverage and large residuals are particularly influential."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plot(hatvalues(races.lm), rstandard(races.lm), pch=23, bg='red', cex=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "`R` will put the IDs of cases that seem to be influential in these (and other plots). Not surprisingly, we see our usual three suspects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plot(races.lm, which=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Influence measures\n",
    "\n",
    "As mentioned above, `R` has its own rules for flagging points as being influential. To\n",
    "see a summary of these, one can use the `influence.measures` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "influence.measures(races.lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "While not specified in the documentation, the meaning of the asterisks can be found\n",
    "by reading the code. The function `is.influential` makes the decisions\n",
    "to flag cases as influential or not. \n",
    "\n",
    "- We see that the `DFBETAS` are thresholded at 1.\n",
    "\n",
    "- We see that `DFFITS` is thresholded at `3 * sqrt((p+1)/(n-p-1))`.\n",
    "\n",
    "- Etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "influence.measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Problems in the regression function\n",
    "\n",
    "-   True regression function may have higher-order non-linear terms,\n",
    "    polynomial or otherwise.\n",
    "\n",
    "-   We may be missing terms involving more than one ${X}_{(\\cdot)}$,\n",
    "    i.e. ${X}_i \\cdot {X}_j$ (called an *interaction*).\n",
    "\n",
    "-   Some simple plots: *added-variable* and *component plus residual*\n",
    "    plots can help to find nonlinear functions of *one variable*.\n",
    "    \n",
    "- I find these plots of somewhat limited use in practice, but we will go over them as\n",
    "possibly useful diagnostic tools.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Added variable plots\n",
    "\n",
    "- The plots can be helpful for finding influential points, outliers. The functions can \n",
    "be found in the `car` package.\n",
    "\n",
    "-   Procedure:\n",
    "\n",
    "    -   Let $\\tilde{e}_{X_j,i}, 1\\leq i \\leq n$ be the residuals after\n",
    "        regressing $X_j$ onto all columns of $X$ except\n",
    "        $X_j$;\n",
    "\n",
    "    -   Let $e_{X_j,i}$ be the residuals after regressing ${Y}$ onto\n",
    "        all columns of ${X}$ except ${X}_j$;\n",
    "\n",
    "    -   Plot $\\tilde{e}_{X_j}$ against $e_{X_j}$.\n",
    "    \n",
    "    - If the (partial regression) relationship is linear this plot should look linear.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "avPlots(races.lm, 'Distance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "avPlots(races.lm, 'Climb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Component + residual plots\n",
    "\n",
    "-   Similar to added variable, but may be more helpful in identifying nonlinear relationships.\n",
    "\n",
    "-   Procedure: plot $X_{ij}, 1 \\leq i \\leq n$ vs.\n",
    "    $e_i + \\widehat{\\beta}_j \\cdot X_{ij} , 1 \\leq i \\leq n$.\n",
    "   \n",
    "- The green line is a non-parametric smooth of the scatter plot that may suggest\n",
    "relationships other than linear.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "crPlots(races.lm, 'Distance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "crPlots(races.lm, 'Climb')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
