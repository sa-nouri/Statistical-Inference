{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Simple linear regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "options(repr.plot.width=5, repr.plot.height=3)\n",
    "set.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The first type of model, which we will spend a lot of time on, is the *simple linear regresssion model*. One simple way to think of it\n",
    "is via scatter plots. Below are [heights](http://www.stat.cmu.edu/~roeder/stat707/=data/=data/data/Rlibraries/alr3/html/heights.html) of mothers and daughters collected \n",
    "by Karl Pearson in the late 19th century. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "library(alr3)\n",
    "data(heights)\n",
    "M = heights$Mheight\n",
    "D = heights$Dheight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "library(ggplot2)\n",
    "heights_fig = ggplot(heights, aes(Mheight, Dheight)) + geom_point() + theme_bw();\n",
    "heights_fig\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A simple linear regression model fits a line through the above scatter plot in a particular way. Specifically, it tries to estimate\n",
    "the height of a new daughter in this population, say $D_{new}$, whose mother had height $H_{new}$. It does this by considering\n",
    "each slice of the data. Here is a slice of the data near $M=66$, the slice is taken over a window of size 1 inch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X = 66\n",
    "rect = data.frame(xmin=X-.5, xmax=X+.5, ymin=-Inf, ymax=Inf)\n",
    "heights_fig + geom_rect(data=rect, aes(xmin=xmin, xmax=xmax, ymin=ymin, ymax=ymax),\n",
    "                        color=\"grey20\",\n",
    "                        alpha=0.5,\n",
    "                        inherit.aes = FALSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "selected_points = (M <= X+.5) & (M >= X-.5)\n",
    "mean_within_slice = mean(D[selected_points])\n",
    "mean_within_slice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We see that, in our sample, the average height of daughters whose height fell within our slice is about 65.2 inches. Of course this\n",
    "height varies by slice. For instance, at 60 inches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X = 60\n",
    "selected_points = (M <= X+.5) & (M >= X-.5)\n",
    "mean_within_slice = mean(D[selected_points])\n",
    "mean_within_slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X = 60\n",
    "rect = data.frame(xmin=X-.5, xmax=X+.5, ymin=-Inf, ymax=Inf)\n",
    "heights_fig + geom_rect(data=rect, aes(xmin=xmin, xmax=xmax, ymin=ymin, ymax=ymax),\n",
    "                        color=\"grey20\",\n",
    "                        alpha=0.5,\n",
    "                        inherit.aes = FALSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The regression model puts a line through this scatter plot in an *optimal* fashion.\n",
    "\n",
    "To do this, it assumes that the mean in slice `M` lies on some line\n",
    "$$\n",
    "\\beta_0+\\beta_1 M.\n",
    "$$\n",
    "\n",
    "It then chooses $(\\beta_0, \\beta_1)$ based on the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "parameters = lm(D ~ M)$coef\n",
    "print(parameters)\n",
    "intercept = parameters[1]\n",
    "slope = parameters[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "heights_fig + geom_abline(slope=slope, intercept=intercept, color='red', size=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What is a \"regression\" model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A regression model is a model of the relationships between some covariates (predictors) and an outcome. Specifically, regression is a model of the average outcome given the covariates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Mathematical formulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For height of couples data: a mathematical model:\n",
    "$$\n",
    "{\\tt Daughter} = f({\\tt Mother}) + \\varepsilon\n",
    "$$\n",
    "where $f$ gives the average height of the daughter of a mother of height Mother and $\\varepsilon$ is the random variation within the slice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Linear regression models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* A *linear* regression model says that\n",
    "the function $f$ is a sum (linear combination) of functions of ${\\tt Mother}$.\n",
    "\n",
    "* Simple linear regression model:\n",
    "   $$f({\\tt Mother}) = \\beta_0 + \\beta_1 \\cdot {\\tt Mother}$$\n",
    "   for some unknown parameter vector $(\\beta_0, \\beta_1)$.\n",
    "\n",
    "* Could also be a sum (linear combination) of *fixed* functions of `Mother`:\n",
    "   $$f({\\tt Mother}) = \\beta_0 + \\beta_1 \\cdot {\\tt Mother} + \\beta_2 \\cdot {\\tt Mother}^2\n",
    "   $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Simple linear regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    " *  *Simple linear* regression is the case when there is only one predictor:\n",
    "   $$\n",
    "   f({\\tt Mother}) = \\beta_0 + \\beta_1  \\cdot {\\tt Mother}.$$\n",
    "\n",
    "* Let $Y_i$ be the height of the $i$-th daughter in the sample, $X_i$ be the height of the $i$-th mother.\n",
    "\n",
    "* Model:\n",
    "   $$\n",
    "   Y_i = \\underbrace{\\beta_0 + \\beta_1 X_i}_{\\text{regression equation}} + \\underbrace{\\varepsilon_i}_{\\text{error}}$$\n",
    "   where $\\varepsilon_i \\sim N(0, \\sigma^2)$ are independent.\n",
    "\n",
    "* This specifies a *distribution* for the $Y$'s given the $X$'s, i.e.\n",
    "   it is a *statistical model*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Fitting the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* We will be using *least squares* regression. This measures\n",
    "   the *goodness of fit* of a line by the sum of squared errors, $SSE$.\n",
    "   \n",
    "* Least squares regression chooses the line that minimizes\n",
    "   $$\n",
    "   SSE(\\beta_0, \\beta_1) = \\sum_{i=1}^n (Y_i - \\beta_0 - \\beta_1 \\cdot X_i)^2.$$\n",
    "\n",
    "* In principle, we might measure goodness of fit differently: \n",
    "   $$\n",
    "   SAD(\\beta_0, \\beta_1) = \\sum_{i=1}^n |Y_i - \\beta_0 - \\beta_1 \\cdot X_i|.$$\n",
    "   \n",
    "* For some *loss function* $L$ we might try to minimize\n",
    "    $$\n",
    "    L(\\beta_0,\\beta_1) = \\sum_{i=1}^n L(Y_i-\\beta_0-\\beta_1X_i) \n",
    "    $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Why least squares?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* With least squares, the minimizers have explicit formulae -- not so important with today's computer power -- especially when $L$ is convex.\n",
    "\n",
    "* Resulting formulae are *linear* in the outcome $Y$. This is important\n",
    "   for inferential reasons. For only predictive power, this is also not so important.\n",
    "   \n",
    "* If assumptions are correct, then this is *maximum likelihood estimation*.\n",
    "\n",
    "* Statistical theory tells us the *maximum likelihood estimators (MLEs)* are generally good estimators.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Choice of loss function\n",
    "\n",
    "The choice of the function we use to measure goodness of fit, or the *loss* function, has an outcome on what\n",
    "sort of estimates we get out of our procedure. For instance, if, instead of fitting a line to a scatterplot, we were\n",
    "estimating a *center* of a distribution, which we denote by $\\mu$, then we might consider minimizing several loss functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Choice of loss function\n",
    "\n",
    "* If we choose the sum of squared errors:\n",
    "$$\n",
    "SSE(\\mu) = \\sum_{i=1}^n (Y_i - \\mu)^2.\n",
    "$$\n",
    "Then, we know that the minimizer of $SSE(\\mu)$ is the sample mean.\n",
    "\n",
    "* On the other hand, if we choose the sum of the absolute errors\n",
    " $$\n",
    "   SAD(\\mu) = \\sum_{i=1}^n |Y_i - \\mu|.$$\n",
    "   Then, the resulting minimizer is the sample median.\n",
    "   \n",
    "* Both of these minimization problems also have *population* versions as well. For instance, the population mean\n",
    "minimizes, as a function of $\\mu$\n",
    "$$\n",
    "\\mathbb{E}((Y-\\mu)^2)\n",
    "$$\n",
    "while the population median minimizes\n",
    "$$\n",
    "\\mathbb{E}(|Y-\\mu|).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Visualizing the loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let's take some a random scatter plot and view the loss function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X = rnorm(50)\n",
    "Y = 1.5 + 0.1 * X + rnorm(50) * 2\n",
    "parameters = lm(Y ~ X)$coef\n",
    "intercept = parameters[1]\n",
    "slope = parameters[2]\n",
    "ggplot(data.frame(X, Y), aes(X, Y)) + geom_point() + geom_abline(slope=slope, intercept=intercept)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's plot the *loss* as a function of the parameters. Note that the\n",
    "*true* intercept is 1.5 while the *true* slope is 0.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "grid_intercept = seq(intercept - 2, intercept + 2, length=100)\n",
    "grid_slope = seq(slope - 2, slope + 2, length=100)\n",
    "loss_data = expand.grid(intercept_=grid_intercept, slope_=grid_slope)\n",
    "loss_data$squared_error = numeric(nrow(loss_data))\n",
    "for (i in 1:nrow(loss_data)) {\n",
    "    loss_data$squared_error[i] = sum((Y - X * loss_data$slope_[i] - loss_data$intercept_[i])^2)\n",
    "}\n",
    "squared_error_fig = (ggplot(loss_data, aes(intercept_, slope_, fill=squared_error)) + \n",
    "                     geom_raster() +\n",
    "                     scale_fill_gradientn(colours=c(\"gray\",\"yellow\",\"blue\")))\n",
    "squared_error_fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's contrast this with the sum of absolute errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "loss_data$absolute_error = numeric(nrow(loss_data))\n",
    "for (i in 1:nrow(loss_data)) {\n",
    "    loss_data$absolute_error[i] = sum(abs(Y - X * loss_data$slope_[i] - loss_data$intercept_[i]))\n",
    "}\n",
    "absolute_error_fig = (ggplot(loss_data, aes(intercept_, slope_, fill=absolute_error)) + \n",
    "                      geom_raster() +\n",
    "                      scale_fill_gradientn(colours=c(\"gray\",\"yellow\",\"blue\")))\n",
    "absolute_error_fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Geometry of least squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The following picture will be with us, in various guises, throughout much of the course. It depicts\n",
    "the geometric picture involved in least squares regression.\n",
    "\n",
    "<img src=\"http://stats191.stanford.edu/figs/axes_simple.svg\" width=\"600\">\n",
    "\n",
    "It requires some imagination but the picture should be thought as representing vectors in $n$-dimensional space, l where $n$ is the number of points in the scatterplot. In our height data, $n=1375$. The bottom two axes should be thought of as 2-dimensional, while the axis marked \"$\\perp$\" should be thought of as $(n-2)$ dimensional, or, 1373 in this case.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Important lengths\n",
    "\n",
    "The (squared) lengths of the above vectors are important quantities in what follows.\n",
    "\n",
    "There are three to note:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "   SSE &= \\sum_{i=1}^n(Y_i - \\widehat{Y}_i)^2 = \\sum_{i=1}^n (Y_i - \\widehat{\\beta}_0 - \\widehat{\\beta}_1 X_i)^2 \\\\\n",
    "   SSR &= \\sum_{i=1}^n(\\overline{Y} - \\widehat{Y}_i)^2 = \\sum_{i=1}^n (\\overline{Y} - \\widehat{\\beta}_0 - \\widehat{\\beta}_1 X_i)^2 \\\\\n",
    "   SST &= \\sum_{i=1}^n(Y_i - \\overline{Y})^2 = SSE + SSR \\\\\n",
    "   R^2 &= \\frac{SSR}{SST} = 1 - \\frac{SSE}{SST} = \\widehat{Cor}(\\pmb{X},\\pmb{Y})^2.\n",
    "   \\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Important lengths\n",
    "\n",
    "\n",
    "An important summary of the fit is the ratio\n",
    "$$\n",
    "R^2 = \\frac{SSR}{SST} = 1 - \\frac{SSE}{SST}\n",
    "$$\n",
    "which measures *how much variability in $Y$* is explained by $X$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example: wages vs. education"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "In this example, we'll look at the output of *lm* for the wage\n",
    "data and verify that some of the equations we present for the \n",
    "least squares solutions agree with the output.\n",
    "The data was compiled from a study in econometrics [Learning about Heterogeneity in Returns to Schooling]( http://www.econ.queensu.ca/jae/2004-v19.7/koop-tobias/readme.kt.txt).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "url = 'http://www.stanford.edu/class/stats191/data/wage.csv'\n",
    "wages = read.table(url, sep=',', header=TRUE)\n",
    "print(head(wages))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In order to access the variables in `wages` we `attach` it so that the variables\n",
    "are in the toplevel namespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "attach(wages)\n",
    "mean(logwage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's fit the linear regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "wages.lm = lm(logwage ~ education)\n",
    "print(wages.lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "As in the mother-daughter data, we might want to plot the data and add the regression line.\n",
    "\n",
    "Earlier, we used `ggplot2`, below we use base `R` instead. Typically `ggplot2` will\n",
    "be more attractive, though its result are sometimes a little difficult to tweak (in my limited experience)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "logwage_fig = (ggplot(wages, aes(education, logwage)) + geom_point() + theme_bw() +\n",
    "               geom_abline(slope=wages.lm$coef[2], \n",
    "                           intercept=wages.lm$coef[1], \n",
    "                           color='red', \n",
    "                           size=3))\n",
    "logwage_fig\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Least squares estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "There are explicit formulae for the least squares estimators, i.e. the minimizers of the error sum of squares.\n",
    "\n",
    "For the slope, $\\hat{\\beta}_1$, it can be shown that \n",
    "$$\n",
    "   \\widehat{\\beta}_1 = \\frac{\\sum_{i=1}^n(X_i - \\overline{X})(Y_i - \\overline{Y}\n",
    ")}{\\sum_{i=1}^n (X_i-\\overline{X})^2} = \\frac{\\widehat{Cov}(X,Y)}{\\widehat{Var}(\n",
    "X)}.$$\n",
    "\n",
    "Knowing the slope estimate, the intercept estimate can be found easily:\n",
    "$$ \\widehat{\\beta}_0 = \\overline{Y} - \\widehat{\\beta}_1 \\cdot \\overline{\n",
    "X}.$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Wages example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "beta.1.hat = cov(education, logwage) / var(education)\n",
    "beta.0.hat = mean(logwage) - beta.1.hat * mean(education)\n",
    "print(c(beta.0.hat, beta.1.hat))\n",
    "print(coef(wages.lm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Estimate of $\\sigma^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "There is one final quantity needed to estimate all of our parameters in our (statistical) model for the scatterplot. This is $\\sigma^2$,\n",
    "the variance of the random variation within each slice (the regression model assumes this variance is constant within each slice...).\n",
    "\n",
    "The estimate most commonly used is\n",
    "$$\n",
    "\\hat{\\sigma}^2 = \\frac{1}{n-2} \\sum_{i=1}^n (Y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 X_i)^2 = \\frac{SSE}{n-2} = MSE\n",
    "$$\n",
    "\n",
    "Above, note the practice of replacing the quantity $SSE(\\hat{\\beta}_0,\\hat{\\beta}_1)$, i.e. the minimum of this function, with just $SSE$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The term *MSE* above refers to mean squared error: a sum of squares divided by what we call its *degrees of freedom*. The degrees of freedom\n",
    "of *SSE*, the *error sum of squares* is therefore $n-2$. Remember this $n-2$ corresponded to $\\perp$ in the picture above...\n",
    "\n",
    "Using some statistical calculations that we will not dwell on, if our simple linear regression model is correct, then we can see that\n",
    "$$\n",
    "\\frac{\\hat{\\sigma}^2}{\\sigma^2} \\sim \\frac{\\chi^2_{n-2}}{n-2}\n",
    "$$\n",
    "where the right hand side denotes a *chi-squared* distribution with $n-2$ degrees of freedom.\n",
    "\n",
    "(Note: our estimate of $\\sigma^2$ *is not* the maximum likelihood estimate.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Wages example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sigma.hat = sqrt(sum(resid(wages.lm)^2) / wages.lm$df.resid)\n",
    "c(sigma.hat, sqrt(sum((logwage - predict(wages.lm))^2) / wages.lm$df.resid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The summary from *R* also contains this estimate of $\\sigma$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "summary(wages.lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Inference for the simple linear regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### What do we mean by inference?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Generally, by inference, we mean \"learning something about\n",
    "   the relationship between the sample $(X_1, \\dots, X_n)$ and $(Y_1, \\dots, Y_n)$.\"\n",
    "\n",
    "* In the simple linear regression model, this often means learning about $\\beta_0, \\beta_1$.\n",
    "Particular forms of inference are **confidence intervals** or **hypothesis tests**. More on these later.\n",
    "\n",
    "* Most of the questions of *inference* in this course\n",
    "   can be answered in terms of $t$-statistics or $F$-statistics.\n",
    "\n",
    "* First we will talk about $t$-statistics, later $F$-statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Examples of (statistical) hypotheses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* [One sample problem:](http://en.wikipedia.org/wiki/Student%27s_t-test#One-sample_t-test) given an independent sample $\\pmb{X}=(X_1, \\dots, X_n)$ where $X_i\\sim N(\\mu,\\sigma^2)$, the *null hypothesis $H_0:\\mu=\\mu_0$*  says that in fact the population mean is some specified value $\\mu_0$.\n",
    "\n",
    "* [Two sample problem:](http://en.wikipedia.org/wiki/Student%27s_t-test#Independent_two-sample_t-test) given two independent samples $\\pmb{Z}=(Z_1, \\dots, Z_n)$, $\\pmb{W}=(W_1, \\dots, W_m)$  where $Z_i\\sim N(\\mu_1,\\sigma^2)$ and $W_i \\sim N(\\mu_2, \\sigma^2)$, the *null hypothesis $H_0:\\mu_1=\\mu_2$* says that in fact the population means from which the two samples are drawn are identical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Testing a hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We test a null hypothesis, $H_0$ based on some test statistic $T$ whose distribution is fully known when $H_0$ is true.\n",
    "\n",
    "For example, in the one-sample problem, if $\\bar{X}$ is the sample mean of our sample $(X_1, \\dots, X_n)$ and\n",
    "$$\n",
    "S^2 = \\frac{1}{n-1} \\sum_{i=1}^n (X_i-\\bar{X})^2\n",
    "$$\n",
    "is the sample variance. Then\n",
    "$$\n",
    "T = \\frac{\\bar{X}-\\mu_0}{S/\\sqrt{n}}\n",
    "$$\n",
    "has what is called a [Student's t](http://en.wikipedia.org/wiki/Student's_t-distribution) distribution with $n-1$ degrees of freedom *when $H_0:\\mu=\\mu_0$ is true.* \n",
    "\n",
    "**When the null\n",
    "hypothesis is not true, it does not have this distribution!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### General form of a (Student's) $T$ statistic\n",
    "\n",
    "* A $t$ statistic with $k$ degrees of freedom, has a form that becomes easy to recognize after seeing it several times. \n",
    "\n",
    "* It has two main parts: a numerator and a denominator. The numerator $Z \\sim N(0,1)$ while\n",
    "$D \\sim \\sqrt{\\chi^2_k/k}$ that is assumed *independent* of $Z$.\n",
    "\n",
    "* The $t$-statistic has the form\n",
    "$$\n",
    "T = \\frac{Z}{D}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### General form of a (Student's) $T$ statistic\n",
    "\n",
    "* Another form of the $t$-statistic is\n",
    "$$\n",
    "T = \\frac{\\text{estimate of parameter} - \\text{true parameter}}{\\text{accuracy of the estimate}}.\n",
    "$$\n",
    "\n",
    "* In more formal terms, we write this as\n",
    "$$\n",
    "T = \\frac{\\hat{\\theta} - \\theta}{SE(\\hat{\\theta})}.\n",
    "$$\n",
    "Note that the denominator is the accuracy of the *estimate* and not the \"accuracy\" of the true parameter (which is usually assumed fixed, though not for Bayesians).\n",
    "\n",
    "- The term $SE$ or *standard error* will, in this course, usually refer to an estimate of the accuracy of estimator. Therefore, it is the square root of an estimate of the variance of an estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### General form of a (Student's) $T$ statistic\n",
    "\n",
    "\n",
    "* In our simple linear regression model, a natural (**unobservable**) $t$-statistic is\n",
    "$$\n",
    "\\frac{\\hat{\\beta}_1 - \\beta_1}{SE(\\hat{\\beta}_1)}.\n",
    "$$\n",
    "We've seen how to compute $\\hat{\\beta}_1$, we never get to see the true $\\beta_1$, so the only quantity we have anything left to say about is the standard error $SE(\\hat{\\beta}_1)$. \n",
    "\n",
    "* How many degrees of freedom would this $T$ have?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Comparison of Student's $t$ to normal distribution\n",
    "\n",
    "As the degrees of freedom increases, the population histogram, or density, of the $T_k$ distribution looks more and more\n",
    "like the standard normal distribution usually denoted by $N(0,1)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "rejection_region = function(dens, q_lower, q_upper, xval) {\n",
    "    fig = (ggplot(data.frame(x=xval), aes(x)) +\n",
    "        stat_function(fun=dens, geom='line') +\n",
    "        stat_function(fun=function(x) {ifelse(x > q_upper | x < q_lower, dens(x), NA)},\n",
    "                      geom='area', fill='#CC7777') + \n",
    "            labs(y='Density', x='T') +\n",
    "        theme_bw())\n",
    "}\n",
    "\n",
    "xval = seq(-4,4,length=101)\n",
    "q = qnorm(0.975)\n",
    "Z_fig = rejection_region(dnorm, -q, q, xval) + \n",
    "          annotate('text', x=2.5, y=dnorm(2)+0.3, label='Z statistic',\n",
    "                  color='#CC7777')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This change in the density has an effect on the *rejection rule* for hypothesis tests based on the $T_k$ distribution.\n",
    "For instance, for the standard normal, the 5% rejection rule is to reject if the so-called $Z$-score is larger than about 2 in absolute value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "Z_fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For the $T_{10}$ distribution, however, this rule must be modified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "q10 = qt(0.975, 10)\n",
    "T_fig = (Z_fig + stat_function(fun=function(x) {ifelse(x > q10 | x < -q10, dt(x, 10), NA)},\n",
    "                      geom='area', fill='#7777CC', alpha=0.5) +\n",
    "         stat_function(fun=function(x) {dt(x, 10)}, color='blue') + \n",
    "         annotate('text', x=2.5, y=dnorm(2)+0.27, label='T statistic, df=10',\n",
    "                  color='#7777CC')\n",
    "        );\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "T_fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### One sample problem revisited"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Above, we used the one sample problem as an example of a $t$-statistic. Let's be a little more specific.\n",
    "\n",
    "* Given an independent sample $\\pmb{X}=(X_1, \\dots, X_n)$ where $X_i\\sim N(\\mu,\\sigma^2)$ we can test $H_0:\\mu=0$ using a $T$-statistic.\n",
    "\n",
    "* We can prove that the random variables\n",
    "   $$\\overline{X} \\sim N(\\mu, \\sigma^2/n), \\qquad \\frac{S^2_X}{\\sigma^2} \\sim \\frac{\\chi^2_{n-1}}{n-1}$$\n",
    "   are independent.\n",
    "\n",
    "* Therefore, whatever the true $\\mu$ is\n",
    "   $$\n",
    "   \\frac{\\overline{X} - \\mu}{S_X / \\sqrt{n}} = \\frac{ (\\overline{X}-\\mu) / (\\sigma/\\sqrt{n})}{S_X / \\sigma} \\sim t_{n-1}.$$\n",
    "  \n",
    "* Our null hypothesis specifies a particular value for $\\mu$, i.e. 0. Therefore, under $H_0:\\mu=0$ (i.e. assuming that $H_0$ is true), $$\\overline{X}/(S_X/\\sqrt{n}) \\sim t_{n-1}.$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Confidence interval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The following are examples of confidence intervals we saw in our review.\n",
    "\n",
    "* One sample problem: instead of deciding whether $\\mu=0$, we might want \n",
    "to come up with an (random) interval $[L,U]$ based on the sample $\\pmb{X}$ such \n",
    "that the probability\n",
    "   the true (nonrandom) $\\mu$ is contained in $[L,U]$ equal to $1-\\alpha$, i.e. \n",
    "95%.\n",
    "\n",
    "*  Two sample problem: find a (random) interval $[L,U]$ based on the sampl\n",
    "es $\\pmb{Z}$ and $\\pmb{W}$ such that\n",
    "   the probability the true (nonrandom) $\\mu_1-\\mu_2$ is contained in $[L,U]$ is\n",
    " equal to $1-\\alpha$, i.e. 95%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Confidence interval for one sample problem\n",
    "\n",
    "* In the one sample problem, we might be interested in a confidence interval for the unknown $\\mu$.\n",
    "\n",
    "* Given an independent sample $(X_1, \\dots, X_n)$ where \n",
    "   $X_i\\sim N(\\mu,\\sigma^2)$ we can test construct \n",
    "   a $(1-\\alpha)*100\\%$ using the\n",
    "   numerator and denominator of the $t$-statistic.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Confidence interval for one sample problem\n",
    "\n",
    "*   Let $q=t_{n-1,(1-\\alpha/2)}$\n",
    "\n",
    "   $$\n",
    "   \\begin{aligned}\n",
    "   1 - \\alpha &= P_{\\mu}\\left(-q \\leq \\frac{\\mu - \\overline{X}}\n",
    "   {S_X / \\sqrt{n}} \\leq q \\right) \\\\\n",
    "   &= P_{\\mu}\\left(-q \\cdot {S_X / \\sqrt{n}} \\leq {\\mu - \\overline{X}} \n",
    "   \\leq q  \\cdot {S_X / \\sqrt{n}} \\right) \\\\\n",
    "   &= P_{\\mu}\\left(\\overline{X} - q  \\cdot {S_X / \\sqrt{n}} \n",
    "   \\leq {\\mu} \\leq \\overline{X} + q  \\cdot {S_X / \\sqrt{n}} \\right) \\\\\n",
    "   \\end{aligned}\n",
    "   $$\n",
    "   \n",
    "* Therefore, the interval $\\overline{X} \\pm q \\cdot {S_X / \\sqrt{n}}$ is a $(1-\\alpha)*100\\%$ confidence interval for $\\mu$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Inference for $\\beta_0$ or $\\beta_1$\n",
    "\n",
    "* Recall our model $$\n",
    "   Y_i = \\beta_0 + \\beta_1 X_i + \\varepsilon_i,$$\n",
    "   errors $\\varepsilon_i$ are independent $N(0, \\sigma^2)$.\n",
    "   \n",
    "* In our heights example, we might want to now if there\n",
    "   really is a linear association between ${\\tt Daughter}=Y$\n",
    "   and ${\\tt Mother}=X$. This can be answered with a *hypothesis test* of the null hypothesis $H_0:\\beta_1=0$.\n",
    "   This assumes the model above is correct, but that $\\beta_1=0$.\n",
    "   \n",
    "* Alternatively, we might want to have a range of values that we can be fairly certain $\\beta_1$ lies within.\n",
    "This is a *confidence interval* for $\\beta_1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Geometric picture of model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The hypothesis test has a geometric interpretation which we will revisit later for other models.\n",
    "It is a comparison of two models. The first model is our original model.\n",
    "\n",
    "<img src=\"http://stats191.stanford.edu/figs/axes_simple_full.svg\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Setup for inference\n",
    "\n",
    "* Let $L$ be the subspace of $\\mathbb{R}^n$ spanned $\\pmb{1}=(1, \\dots, 1)$ and ${X}=(X_1, \\dots, X\\\n",
    "_n)$.\n",
    "\n",
    "* Then,\n",
    "   $${Y} = P_L{Y} + ({Y} - P_L{Y}) = \\widehat{{Y}} + (Y - \\widehat{{Y}}) = \\widehat{{Y}} + e$$\n",
    "\n",
    "* In our model $\\mu=\\beta_0 \\pmb{1} + \\beta_1 {X} \\in L$ so that\n",
    "   $$\n",
    "   \\widehat{{Y}} = \\mu + P_L{\\varepsilon}, \\qquad {e} = P_{L^{\\perp}}{{Y}} = P_{L^{\\perp}}{\\varepsilon}$$\n",
    " \n",
    "* Our assumption that $\\varepsilon_i$'s are independent $N(0,\\sigma^2)$ tells us that: ${e}$ and $\\widehat{{Y}}$ are independent; $\\widehat{\\sigma}^2 = \\|{e}\\|^2 / (n-2) \\sim \\sigma^2 \\cdot \\chi^2_{n-2} / (n-2)$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Setup for inference\n",
    "\n",
    "* In turn, this implies\n",
    "$$\n",
    "   \\widehat{\\beta}_1 \\sim N\\left(\\beta_1, \\frac{\\sigma^2}{\\sum_{i=1}^n(X_i-\\overline{X})^2}\\right).$$\n",
    "\n",
    "* Therefore, $$\\frac{\\widehat{\\beta}_1 - \\beta_1}{\\sigma \\sqrt{\\frac{1}{\\sum_{i=1}^n(X_i-\\overline{X})^2}}} \\sim N(\\\n",
    "0,1).$$\n",
    "\n",
    "* The other quantity we need is the *standard error* or SE of $\\hat{\\beta}_1$. This is\n",
    "obtained from estimating the variance of $\\widehat{\\beta}_1$, which, in this case means simply\n",
    "plugging in our estimate of $\\sigma$, yielding\n",
    "$$\n",
    "   SE(\\widehat{\\beta}_1) = \\widehat{\\sigma} \\sqrt{\\frac{1}{\\sum_{i=1}^n(X_i-\\overline{X})^2}} \\qquad \n",
    "   \\text{independent of $\\widehat{\\beta}_1$}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Testing $H_0:\\beta_1=\\beta_1^0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Suppose we want to test that $\\beta_1$ is some pre-specified\n",
    "   value, $\\beta_1^0$ (this is often 0: i.e. is there a linear association)\n",
    "\n",
    "* Under $H_0:\\beta_1=\\beta_1^0$\n",
    "   $$\\frac{\\widehat{\\beta}_1 - \\beta^0_1}{\\widehat{\\sigma} \\sqrt{\\frac{1}{\\sum_{i=1}^n(X_i-\\overline{X})^2}}}\n",
    "   = \\frac{\\widehat{\\beta}_1 - \\beta^0_1}{ \\frac{\\widehat{\\sigma}}{\\sigma}\\cdot \\sigma \\sqrt{\\frac{1}{\n",
    "\\sum_{i=1}^n(X_i-\\overline{X})^2}}} \\sim t_{n-2}.$$\n",
    "\n",
    "\n",
    "* Reject $H_0:\\beta_1=\\beta_1^0$ if $|T| > t_{n-2, 1-\\alpha/2}$.\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Wage example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let's perform this test for the wage data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "SE.beta.1.hat = (sigma.hat * sqrt(1 / sum((education - mean(education))^2)))\n",
    "Tstat = (beta.1.hat - 0) / SE.beta.1.hat\n",
    "data.frame(beta.1.hat, SE.beta.1.hat, Tstat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's look at the output of the `lm` function again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "summary(wages.lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We see that *R* performs this test in the second row of the `Coefficients` table. It is clear that\n",
    "wages are correlated with education."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Why reject for large |T|?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Observing a large $|T|$ is unlikely if $\\beta_1 = \\beta_1^0$: reasonable to conclude that $H_0$ \n",
    "is false.\n",
    "\n",
    "* Common to report $p$-value:\n",
    "$$\\mathbb{P}(|T_{n-2}| > |T|_{obs}) = 2 \\mathbb{P} (T_{n-2} > |T_{obs}|)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "2*(1 - pt(Tstat, wages.lm$df.resid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "detach(wages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Confidence interval based on Student's $t$ distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "*   Suppose we have a parameter estimate $\\widehat{\\theta} \\sim N(\\theta, {\\sigma}_{\\theta}^2)$, and standard error $SE(\\widehat{\\theta})$ such that\n",
    "   $$\n",
    "   \\frac{\\widehat{\\theta}-\\theta}{SE(\\widehat{\\theta})} \\sim t_{\\nu}.$$\n",
    "\n",
    "* We can find a $(1-\\alpha) \\cdot 100 \\%$ confidence interval by:\n",
    "   $$\n",
    "   \\widehat{\\theta} \\pm SE(\\widehat{\\theta}) \\cdot t_{\\nu, 1-\\alpha/2}.$$\n",
    "   \n",
    "* To prove this, expand the absolute value as we did for the one-sample CI\n",
    "   $$\n",
    "   1 - \\alpha = \\mathbb{P}_{\\theta}\\left(\\left|\\frac{\\widehat{\\theta} - \\theta}{SE(\\widehat{\\theta})} \\right| < t_{\\nu, 1-\\alpha/2}\\right).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Confidence interval for regression parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Applying the above to the parameter $\\beta_1$ yields a confidence interval of the form\n",
    "$$\n",
    "   \\hat{\\beta}_1 \\pm SE(\\hat{\\beta}_1) \\cdot t_{n-2, 1-\\alpha/2}.$$\n",
    "   \n",
    "* We will need to compute $SE(\\hat{\\beta}_1)$. This can be computed using this formula\n",
    "   $$\n",
    "   SE(a_0\\hat{\\beta}_0 + a_1\\hat{\\beta}_1) = \\hat{\\sigma} \\sqrt{\\frac{a_0^2}{n} + \\frac{(a_0\\overline{X} - a_1)^2}{\\sum_{i=1}^n \\left(X_i-\\overline{X}\\right)^2}}$$\n",
    "with $(a_0,a_1) = (0, 1)$.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We also need to find the quantity $t_{n-2,1-\\alpha/2}$. This is defined by\n",
    "$$\n",
    "\\mathbb{P}(T_{n-2} \\geq t_{n-2,1-\\alpha/2}) = \\alpha/2.\n",
    "$$\n",
    "In *R*, this is computed by the function `qt`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "alpha = 0.05\n",
    "n = length(M)\n",
    "qt(1-0.5*alpha, n-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Not surprisingly, this is close to that of the normal distribution, which is a Student's $t$ with $\\infty$ for degrees of freedom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "qnorm(1 - 0.5*alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We will not need to use these explicit formulae all the time, as *R* has some built in functions\n",
    "to compute confidence intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "L = beta.1.hat - qt(0.975, wages.lm$df.resid) * SE.beta.1.hat\n",
    "U = beta.1.hat + qt(0.975, wages.lm$df.resid) * SE.beta.1.hat\n",
    "data.frame(L, U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "confint(wages.lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Predicting the mean\n",
    "\n",
    "Once we have estimated a slope $(\\hat{\\beta}_1)$ and an intercept $(\\hat{\\beta}_0)$, we can predict the height\n",
    "of the daughter born to a mother of any particular height by the plugging-in the height of the new mother, $M_{new}$ into\n",
    "our regression equation:\n",
    "$$\n",
    "E[{D}_{new}] = {\\beta}_0  +{\\beta}_1 M_{new}.\n",
    "$$\n",
    "This equation says that our best guess at the height of the new daughter born to a mother of height $M_{new}$ is $\\hat{D}_{new}$. \n",
    "Does this say that the height will be *exactly* this value? No, there is some random variation in each slice, and we would expect the same random variation for this new daughter's height as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " We might also want a confidence interval for the average height of daughters born to a mother of height $M_{new}=66$ inches:\n",
    "$$\n",
    "\\hat{\\beta}_0 + 66 \\cdot \\hat{\\beta}_1 \\pm SE(\\hat{\\beta}_0 + 66 \\cdot \\hat{\\beta}_1) \\cdot t_{n-2, 1-\\alpha/2}.\n",
    "$$\n",
    "\n",
    "Recall that the parameter of interest is the average within the slice. Let's look at our picture again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "height.lm = lm(D~M)\n",
    "predict(height.lm, list(M=c(66, 60)), interval='confidence', level=0.90)\n",
    "heights_fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Computing $SE(\\hat{\\beta}_0 + 66 \\hat{\\beta}_1)$\n",
    "\n",
    "- We use the previous formula\n",
    "  $$\n",
    "   SE(a_0\\hat{\\beta}_0 + a_1\\hat{\\beta}_1) = \\hat{\\sigma} \\sqrt{\\frac{a_0^2}{n} + \\frac{(a_0\\overline{X} - a_1)^2}{\\sum_{i=1}^n \\left(X_i-\\overline{X}\\right)^2}}$$\n",
    "   with $(a_0, a_1) = (1, 66)$.\n",
    "   \n",
    "- In particular,\n",
    "$$\n",
    " SE(\\hat{\\beta}_0 + 66 \\hat{\\beta}_1) = \\hat{\\sigma} \\sqrt{\\frac{1}{n} + \\frac{(\\overline{X} - 66)^2}{\\sum_{i=1}^n \\left(X_i-\\overline{X}\\right)^2}}\n",
    " $$\n",
    "  \n",
    "- As $n$ grows, $SE(\\hat{\\beta}_0 + 66 \\hat{\\beta}_1)$ should shrink to 0. Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Forecasting intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "There is yet another type of interval we might consider: can we find an interval that covers the height of a \n",
    "particular daughter knowing only that her mother's height as 66 inches?\n",
    "\n",
    "This interval has to cover the variability of the new random variation with our slice at 66 inches. So, it must be at least\n",
    "as wide as $\\sigma$, and we estimate its width to be at least as wide as $\\hat{\\sigma}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X = 66\n",
    "selected_points = (M <= X+.5) & (M >= X-.5)\n",
    "center = mean(D[selected_points])\n",
    "sd_ = sd(D[selected_points])\n",
    "L = center - qnorm(0.95) * sd_\n",
    "U = center + qnorm(0.95) * sd_\n",
    "data.frame(center, L, U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "predict(height.lm, list(M=66), interval='prediction', level=0.90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "(69.41-61.94)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "With so much \n",
    "data in our heights example, this 90% interval will have width roughly `2 * qnorm(0.95) * sigma.hat.height`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sigma.hat.height = sqrt(sum(resid(height.lm)^2) / height.lm$df.resid)\n",
    "2 * qnorm(0.95) * sigma.hat.height"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The actual width will depend on how accurately we have estimated $(\\beta_0, \\beta_1)$ as well as $\\hat{\\sigma}$. Here is the\n",
    "full formula. Again it is based on the $t$ distribution, the only thing we need to change is what we use for the SE.\n",
    "\n",
    "$$\n",
    "SE(\\widehat{\\beta}_0 + \\widehat{\\beta}_1 66 + \\varepsilon_{\\text{new}}) = \\widehat{\\sigma} \\sqrt{1 + \\frac{1}{n} + \\frac{(\\overline{X} - 66)^2}{\\sum_{i=1}^n \\left(X_i-\\overline{X}\\right)^2}}.\n",
    "$$\n",
    "\n",
    "The final interval is\n",
    "$$ \\hat{\\beta}_0 +  \\hat{\\beta}_1 66 \\pm t_{n-2, 1-\\alpha/2} \\cdot SE(\\hat{\\beta}_0 + \\hat{\\beta}_1 66 + \\varepsilon_{\\text{new}}).\n",
    "   $$"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
