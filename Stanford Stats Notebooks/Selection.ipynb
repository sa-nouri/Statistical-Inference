{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Model selection\n",
    "\n",
    "In a given regression situation, there are often many choices to\n",
    "be made. Recall our usual setup\n",
    "$$\n",
    "Y_{n \\times 1} = X_{n \\times p} \\beta_{p \\times 1} + \\epsilon_{n \\times 1}.\n",
    "$$\n",
    "\n",
    "Any *subset $A \\subset \\{1, \\dots, p\\}$* yields a new regression model\n",
    "$$\n",
    "{\\cal M}(A): Y_{n \\times 1} = X[,A] \\beta[A] + \\epsilon_{n \\times 1}\n",
    "$$\n",
    "by setting $\\beta[A^c]=0$.\n",
    "\n",
    "**Model selection** is, roughly speaking, how to choose $A$ among the\n",
    "$2^p$ possible choices.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "options(repr.plot.width=5, repr.plot.height=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Election data\n",
    "\n",
    "Here is a dataset from the book that we will use to explore different model selection approaches.\n",
    "\n",
    "Variable | Description\n",
    "--- | ---\n",
    "$V$ | votes for a presidential candidate\n",
    "$I$ | are they incumbent?\n",
    "$D$ | Democrat or Republican incumbent?\n",
    "$W$ | wartime election?\n",
    "$G$ | GDP growth rate in election year\n",
    "$P$ | (absolute) GDP deflator growth rate\n",
    "$N$ | number of quarters in which GDP growth rate $> 3.2\\%$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "url = 'http://stats191.stanford.edu/data/election.table'\n",
    "election.table = read.table(url, header=T)\n",
    "pairs(election.table[,2:ncol(election.table)], cex.labels=3, pch=23,\n",
    "      bg='orange', cex=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Problem & Goals\n",
    "\n",
    "* When we have many predictors (with many possible interactions), it can be difficult to find a good model.\n",
    "* Which main effects do we include?\n",
    "* Which interactions do we include?\n",
    "* Model selection procedures try to *simplify / automate* this task.\n",
    "* Election data has $2^6=64$ different models with just main effects!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## General comments\n",
    "\n",
    "- This is generally an \"unsolved\" problem in statistics: there are no magic procedures to get you the \"best model.\"\n",
    "\n",
    "- Many machine learning methods look for good \"sparse\" models: selecting a \"sparse\" model.\n",
    "\n",
    "- \"Machine learning\" often work with very many predictors.\n",
    "\n",
    "- Our model selection problem is generally at a much smaller scale than \"data mining\" problems.\n",
    "\n",
    "- Still, it is a hard problem.\n",
    "\n",
    "- **Inference after selection is full of pitfalls!** \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hypothetical example\n",
    "* Suppose we fit a a model $F: \\quad Y_{n \\times 1} = X_{n \\times p} \\beta_{p \\times 1} + \\varepsilon_{n \\times 1}$ with predictors ${ X}_1, \\dots, { X}_p$.\n",
    "* In reality, some of the $\\beta$’s may be zero. Let’s suppose that $\\beta_{j+1}= \\dots= \\beta_{p}=0$.\n",
    "* Then, any model that includes $\\beta_0, \\dots, \\beta_j$ is *correct*: which model gives the *best* estimates of $\\beta_0, \\dots, \\beta_j$?\n",
    "* Principle of *parsimony* (i.e. Occam’s razor) says that the model with *only* ${X}_1, \\dots, {X}_j$ is \"best\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Justifying parsimony\n",
    "\n",
    "- For simplicity, let’s assume that $j=1$ so there is only one coefficient to estimate.\n",
    "- Then, because each model gives an *unbiased* estimate of $\\beta_1$ we can compare models based on $\\text{Var}(\\widehat{\\beta}_1).$\n",
    "- The best model, in terms of this variance, is the one containing only ${ X}_1$.\n",
    "- What if we didn’t know that only $\\beta_1$ was non-zero (which we don’t know in general)?\n",
    "- In this situation, we must choose a set of variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Model selection: choosing a subset of variables\n",
    "\n",
    "* To \"implement\" a model selection procedure, we first need a criterion or benchmark to compare two models.\n",
    "* Given a criterion, we also need a search strategy.\n",
    "* With a limited number of predictors, it is possible to search all possible models (`leaps` in `R`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Candidate criteria\n",
    "\n",
    "Possible criteria:\n",
    "\n",
    "* $R^2$: not a good criterion. Always increase with model size $\\implies$ \"optimum\" is to take the biggest model.\n",
    "* Adjusted $R^2$: better. It \"penalized\" bigger models. Follows principle of parsimony / Occam’s razor.\n",
    "* Mallow’s $C_p$ – attempts to estimate a model’s predictive power, i.e. the power to predict a new observation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Best subsets, $R^2$\n",
    "\n",
    "Leaps takes a design matrix as argument: throw away the intercept\n",
    "column or leaps will complain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "election.lm = lm(V ~ I + D + W + G:I + P + N, election.table)\n",
    "election.lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X = model.matrix(election.lm)[,-1]\n",
    "library(leaps)\n",
    "election.leaps = leaps(X, election.table$V, nbest=3, method='r2')\n",
    "best.model.r2 = election.leaps$which[which((election.leaps$r2 == \n",
    "                                            max(election.leaps$r2))),]\n",
    "best.model.r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's plot the $R^2$ as a function of the model size. We see that the\n",
    "full model does include all variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plot(election.leaps$size, election.leaps$r2, pch=23, bg='orange', cex=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Best subsets, adjusted $R^2$\n",
    "\n",
    "-   As we add more and more variables to the model – even random ones,\n",
    "    $R^2$ will increase to 1.\n",
    "\n",
    "-   Adjusted $R^2$ tries to take this into account by replacing sums of squares by *mean squares*\n",
    "    $$R^2_a = 1 - \\frac{SSE/(n-p-1)}{SST/(n-1)} = 1 - \\frac{MSE}{MST}.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "election.leaps = leaps(X, election.table$V, nbest=3, method='adjr2')\n",
    "best.model.adjr2 = election.leaps$which[which((election.leaps$adjr2 == max(election.leaps$adjr2))),]\n",
    "best.model.adjr2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plot(election.leaps$size, election.leaps$adjr2, pch=23, bg='orange', \n",
    "     cex=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Mallow’s $C_p$\n",
    "\n",
    "- $C_p({\\cal M}) = \\frac{SSE({\\cal M})}{\\widehat{\\sigma}^2} + 2 \\cdot p({\\cal M}) - n.$\n",
    "- $\\widehat{\\sigma}^2=SSE(F)/df_F$ is the \"best\" estimate of $\\sigma^2$ we have (use the fullest model), i.e. in the election data it uses all 6 main effects.\n",
    "- $SSE({\\cal M})$ is the $SSE$ of the model ${\\cal M}$.\n",
    "- $p({\\cal M})$ is the number of predictors in ${\\cal M}$.\n",
    "- This is an estimate of the expected mean-squared error of $\\widehat{Y}({\\cal M})$, it takes *bias* and *variance* into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "election.leaps = leaps(X, election.table$V, nbest=3, method='Cp')\n",
    "best.model.Cp = election.leaps$which[which((election.leaps$Cp == \n",
    "                                            min(election.leaps$Cp))),]\n",
    "best.model.Cp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plot(election.leaps$size, election.leaps$Cp, pch=23, bg='orange', cex=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Search strategies \n",
    "\n",
    "* Given a criterion, we now have to decide how we are going to search through the possible models.\n",
    "\n",
    "* \"Best subset\": search all possible models and take the one with highest $R^2_a$ or lowest $C_p$ leaps. Such searches are typically\n",
    "feasible only up to $p=30$ or $40$ at the very most.\n",
    "\n",
    "* Stepwise (forward, backward or both): useful when the number of predictors is large. Choose an initial model and be \"greedy\".\n",
    "\n",
    "* \"Greedy\" means always take the biggest jump (up or down) in your selected criterion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Implementations in `R`\n",
    "\n",
    "* \"Best subset\": use the function `leaps`. Works only for multiple linear regression models.\n",
    "* Stepwise: use the function `step`. Works for any model with Akaike Information Criterion (AIC). In multiple linear regression, AIC is (almost) a linear function of $C_p$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Akaike / Bayes Information Criterion\n",
    "\n",
    "* Akaike (AIC) defined as $$AIC({\\cal M}) = - 2 \\log L({\\cal M}) + 2 \\cdot p({\\cal M})$$ where $L({\\cal M})$ is the maximized likelihood of the model.\n",
    "* Bayes (BIC) defined as $$BIC({\\cal M}) = - 2 \\log L({\\cal M}) + \\log n \\cdot p({\\cal M})$$\n",
    "* Strategy can be used for whenever we have a likelihood, so this generalizes to many statistical models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### AIC for regression\n",
    "\n",
    "* In linear regression with unknown $\\sigma^2$ $$-2 \\log L({\\cal M}) = n \\log(2\\pi \\widehat{\\sigma}^2_{MLE}) + n$$ where $\\widehat{\\sigma}^2_{MLE} = \\frac{1}{n} SSE(\\widehat{\\beta})$\n",
    "* In linear regression with known $\\sigma^2$ $$-2 \\log L({\\cal M}) = n \\log(2\\pi \\sigma^2) + \\frac{1}{\\sigma^2} SSE(\\widehat{\\beta})$$ so AIC is very much like Mallow’s $C_p$ in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "n = nrow(X)\n",
    "p = 7 + 1 \n",
    "c(n * log(2*pi*sum(resid(election.lm)^2)/n) + n + 2*p, AIC(election.lm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Properties of AIC / BIC\n",
    "\n",
    "* BIC will typically choose a model as small or smaller than AIC (if using the same search direction).\n",
    "\n",
    "* As our sample size grows, under some assumptions,\n",
    "it can be shown that\n",
    "     - AIC will (asymptotically) always choose a model that contains the true model, i.e. it won’t leave any variables out.\n",
    "     - BIC will (asymptotically) choose exactly the right model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Election example\n",
    "\n",
    "Let's take a look at `step` in action. Probably the simplest\n",
    "strategy is *forward stepwise* which tries to add one variable at a time, \n",
    "as long as it can find a resulting model whose AIC is better than \n",
    "its current position. \n",
    "\n",
    "When it can make no further additions, it terminates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "election.step.forward = step(lm(V ~ 1, election.table), \n",
    "                             list(upper = ~ I + D + W + G + G:I + P + N), \n",
    "                             direction='forward', \n",
    "                             k=2, trace=FALSE)\n",
    "election.step.forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "summary(election.step.forward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Interactions and hierarchy\n",
    "\n",
    "We notice that although the *full* model we gave it had the interaction `I:G`, the function `step` never tried to use it. This is \n",
    "due to some rules implemented in `step` that do not include an interaction unless both main effects are already in the model. In this case, because neither $I$ nor $G$ were added, the interaction was never considered.\n",
    "\n",
    "In the `leaps` example, we gave the function the design matrix\n",
    "and it did not have to consider interactions: they were already encoded in the design matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### BIC example\n",
    "\n",
    "The only difference between AIC and BIC is the price paid\n",
    "per variable. This is the argument `k` to `step`. By default `k=2` and for BIC\n",
    "we set `k=log(n)`. If we set `k=0` it will always add variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "election.step.forward.BIC = step(lm(V ~ 1, election.table), \n",
    "                                 list(upper = ~ I + D + W +G:I + P + N), \n",
    "                                 direction='forward', k=log(nrow(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "summary(election.step.forward.BIC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Backward selection\n",
    "\n",
    "Just for fun, let's consider backwards stepwise. This starts at a full\n",
    "model and tries to delete variables.\n",
    "\n",
    "There is also a `direction=\"both\"` option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "election.step.backward = step(election.lm, direction='backward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "summary(election.step.backward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cross-validation\n",
    "\n",
    "Yet another model selection criterion is \n",
    "$K$-fold cross-validation.\n",
    "\n",
    "- Fix a model ${\\cal M}$. Break data set into $K$ approximately equal sized groups $(G_1, \\dots, G_K)$.\n",
    "- For (i in 1:K) Use all groups except $G_i$ to fit model, predict outcome in group $G_i$ based on this model $\\widehat{Y}_{j,{\\cal M}, G_i}, j \\in G_i$.\n",
    "- Similar to what we saw in Cook's distance / DFFITS.\n",
    "- Estimate $CV({\\cal M}) = \\frac{1}{n}\\sum_{i=1}^K \\sum_{j \\in G_i} (Y_j - \\widehat{Y}_{j,{\\cal M},G_i})^2.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Comments about cross-validation.\n",
    "\n",
    "* It is a general principle that can be used in other situations to \"choose parameters.\"\n",
    "* Pros (partial list): \"objective\" measure of a model's predictive power.\n",
    "* Cons (partial list): all we know about inference is *usually* \"out the window\" (also true for other model selection procedures).\n",
    "* If goal is not really inference about certain specific parameters, it is a reasonable way to compare models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "library(boot)\n",
    "election.glm = glm(V ~ ., data=election.table)\n",
    "cv.glm(model.frame(election.glm), election.glm, K=5)$delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### $C_p$ versus 5-fold cross-validation\n",
    "\n",
    "- Let's plot our $C_p$ versus the $CV$ score.\n",
    "\n",
    "- Keep in mind that there is additional randomness in the $CV$ score\n",
    "due to the random assignments to groups. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "election.leaps = leaps(X, election.table$V, nbest=3, method='Cp')\n",
    "V = election.table$V\n",
    "election.leaps$cv = 0 * election.leaps$Cp\n",
    "for (i in 1:nrow(election.leaps$which)) {\n",
    "    subset = c(1:ncol(X))[election.leaps$which[i,]]\n",
    "    if (length(subset) > 1) {\n",
    "       Xw = X[,subset]\n",
    "       wlm = glm(V ~ Xw)\n",
    "       election.leaps$CV[i] = cv.glm(model.frame(wlm), wlm, K=5)$delta[1]\n",
    "    }\n",
    "    else {\n",
    "       Xw = X[,subset[1]]\n",
    "       wlm = glm(V ~ Xw)\n",
    "       election.leaps$CV[i] = cv.glm(model.frame(wlm), wlm, K=5)$delta[1]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plot(election.leaps$Cp, election.leaps$CV, pch=23, bg='orange', cex=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plot(election.leaps$size, election.leaps$CV, pch=23, bg='orange', cex=2)\n",
    "best.model.Cv = election.leaps$which[which((election.leaps$CV \n",
    "                                            == min(election.leaps$CV))),]\n",
    "best.model.Cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summarizing results\n",
    "\n",
    "The model selected depends on the criterion used.\n",
    "\n",
    "Criterion | Model\n",
    "--- | ---\n",
    "$R^2$ | ~ $ I + D + W +G:I + P + N$\n",
    "$R^2_a$ | ~ $ I + D + P + N$\n",
    "$C_p$ | ~ $D+P+N$\n",
    "AIC forward | ~ $D+P$\n",
    "BIC forward | ~ $D$\n",
    "AIC backward | ~ $I + D + N + I:G$\n",
    "5-fold CV | ~ $ I+W$\n",
    "\n",
    "**The selected model is random and depends on which method we use!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Where we are so far\n",
    "\n",
    "- Many other \"criteria\" have been proposed.\n",
    "- Some work well for some types of data, others for different data.\n",
    "- Check diagnostics!\n",
    "- These criteria (except cross-validation) are not \"direct measures\" of predictive power, though Mallow’s $C_p$ is a step in this direction.\n",
    "- $C_p$ measures the quality of a model based on both *bias* and *variance* of the model. Why is this important?\n",
    "- *Bias-variance* tradeoff is ubiquitous in statistics. More soon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A larger example\n",
    "\n",
    "- Resistance of $n=633$ different HIV+ viruses to drug 3TC.\n",
    "\n",
    "- Features $p=91$ are mutations in a part of the HIV virus, response is log fold change\n",
    "in vitro.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "X_HIV = read.table('http://stats191.stanford.edu/data/NRTI_X.csv', header=FALSE, sep=',')\n",
    "Y_HIV = read.table('http://stats191.stanford.edu/data/NRTI_Y.txt', header=FALSE, sep=',')\n",
    "set.seed(0)\n",
    "Y_HIV = as.matrix(Y_HIV)[,1]\n",
    "X_HIV = as.matrix(X_HIV)\n",
    "nrow(X_HIV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Forward stepwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "D = data.frame(X_HIV, Y_HIV)\n",
    "M = lm(Y_HIV ~ ., data=D)\n",
    "M_forward = step(lm(Y_HIV ~ 1, data=D), list(upper=M), \n",
    "                 trace=FALSE, direction='forward')\n",
    "M_forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Backward stepwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "M_backward = step(M, list(lower= ~  1), trace=FALSE, \n",
    "                  direction='backward')\n",
    "M_backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Both directions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "M_both1 = step(M, list(lower= ~  1, upper=M), trace=FALSE, \n",
    "               direction='both')\n",
    "M_both1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_both2 = step(lm(Y_HIV ~ 1, data=D), list(lower= ~  1, upper=M), trace=FALSE, direction='both')\n",
    "M_both2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Compare selected models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sort(names(coef(M_forward)))\n",
    "sort(names(coef(M_backward)))\n",
    "sort(names(coef(M_both1)))\n",
    "sort(names(coef(M_both2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## BIC vs AIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "M_backward_BIC = step(M, list(lower= ~  1), trace=FALSE, \n",
    "                      direction='backward', k=log(633))\n",
    "M_forward_BIC = step(lm(Y_HIV ~ 1, data=D), list(upper=M), trace=FALSE, \n",
    "                     direction='forward', k=log(633))\n",
    "M_both1_BIC = step(M, list(upper=M, lower=~1), trace=FALSE, \n",
    "                     direction='both', k=log(633))\n",
    "M_both2_BIC = step(lm(Y_HIV ~ 1, data=D), list(upper=M, lower=~1), trace=FALSE, \n",
    "                     direction='both', k=log(633))\n",
    "\n",
    "sort(names(coef(M_backward_BIC)))\n",
    "sort(names(coef(M_forward_BIC)))\n",
    "sort(names(coef(M_both1_BIC)))\n",
    "sort(names(coef(M_both2_BIC)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Inference after selection: data snooping and splitting\n",
    "\n",
    "Each of the above criteria return a model. The `summary` provides\n",
    "$p$-values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "summary(election.step.forward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can also form confidence intervals. **But, can we trust these intervals or tests? No!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "confint(election.step.forward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How bad could it really be?\n",
    "\n",
    "To illustrate the \"dangers\" of trusting the above $p$-values, I will\n",
    "use the `selectiveInference` package which has a variant of forward stepwise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "library(selectiveInference)\n",
    "plot(fs(X, V))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "fsInf(fs(X, V))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's generate data for which we know all coefficients\n",
    "are zero and look at the $p$-values. \n",
    "\n",
    "We will look at the *naive* p-values which ignore selection, as well\n",
    "as a certain kind of corrected $p$-values from `selectiveInference`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X_fake = matrix(rnorm(4000), 100, 40)\n",
    "nsim = 1000\n",
    "naiveP = c()\n",
    "for (i in 1:nsim) {\n",
    "    Z = rnorm(nrow(X_fake))\n",
    "    fsfit = fs(X_fake, Z, maxsteps=1)\n",
    "    fsinf = fsInf(fsfit, sigma=1)\n",
    "    naive.lm = lm(Z ~ X_fake[,fsinf$vars[1]])\n",
    "    naiveP = c(naiveP, summary(naive.lm)$coef[2,4])\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What is the type I error of the naive test?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "ecdf(naiveP)(0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data splitting\n",
    "\n",
    "- A simple fix for this problem is to randomly split the data into two groups (often but not always of equal size).\n",
    "\n",
    "- This gives valid inference under some assumptions.\n",
    "\n",
    "- Which variable is chosen is random as it depends on the split. *Selected model might not be as \"good\".*\n",
    "\n",
    "- Also, we have access to less data to form confidence intervals ($\\implies$ wider), test hypotheses ($\\implies$ less power).\n",
    "\n",
    "## When is data splitting OK?\n",
    "\n",
    "- Just as when we did not select a model, when computing $p$-values and\n",
    "confidence intervals we use a model.\n",
    "\n",
    "- If we are happy with these assumptions for a model selected from the data, then\n",
    "data splitting is OK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "splitP = c()\n",
    "for (i in 1:nsim) {\n",
    "    Z = rnorm(nrow(X_fake))\n",
    "    subset_s = sample(1:nrow(X_fake), nrow(X_fake)/2, replace=FALSE) \n",
    "    #_s for selection\n",
    "    subset_i = rep(TRUE, nrow(X_fake)) # _i for inference\n",
    "    subset_i[subset_s] = FALSE\n",
    "    \n",
    "    # Step 1: choose your model\n",
    "    fsfit = fs(X_fake[subset_s,], Z[subset_s], maxsteps=1)\n",
    "    \n",
    "    # Step 2: compute p-values\n",
    "    split.lm = lm(Z ~ X_fake[,fsinf$vars[1]], subset=subset_i)\n",
    "    splitP = c(splitP, summary(split.lm)$coef[2,4])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plot(ecdf(splitP), lwd=3, col='red')\n",
    "abline(0,1, lwd=2, lty=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "It turns out that it is possible to modify the\n",
    "usual $t$-statistic so that we still get valid tests\n",
    "after the first step. Its description is a little beyond the level of this course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "exactP = c()\n",
    "for (i in 1:nsim) {\n",
    "    Z = rnorm(nrow(X_fake))\n",
    "    fsfit = fs(X_fake, Z, maxsteps=1)\n",
    "    fsinf = fsInf(fsfit, sigma=1)\n",
    "    exactP = c(exactP, fsinf$pv[1])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plot(ecdf(exactP), lwd=3, col='blue', main='Uncorrected vs. corrected null p-values')\n",
    "plot(ecdf(naiveP), lwd=3, col='red', add=TRUE)\n",
    "abline(0,1, lwd=2, lty=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Power\n",
    "\n",
    "- Both the exact and data splitting give valid p-values. \n",
    "\n",
    "- Which is more powerful?\n",
    "\n",
    "- Let's put some signal in and see.\n",
    "\n",
    "- **Note, we can't be sure we are selecting the \"right\" variable each time so model might\n",
    "not be \"correct\". But this can happen even when we write down a model without looking at the data.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "exactP_A = c()\n",
    "splitP_A = c()\n",
    "correct_model = c()\n",
    "\n",
    "beta1 = .3 # signal size\n",
    "\n",
    "for (i in 1:nsim) {\n",
    "    Z = rnorm(nrow(X_fake)) + X_fake[,1] * beta1 \n",
    "    fsfit = fs(X_fake, Z, maxsteps=1)\n",
    "    fsinf = fsInf(fsfit, sigma=1)\n",
    "    exactP_A = c(exactP_A, fsinf$pv[1])\n",
    "    \n",
    "    correct_model = c(correct_model, fsinf$vars[1] == 1)\n",
    "    \n",
    "    # data splitting pvalue\n",
    "    \n",
    "    subset = sample(1:nrow(X_fake), nrow(X_fake)/2, replace=FALSE)\n",
    "    subset_c = rep(TRUE, nrow(X_fake))\n",
    "    subset_c[subset] = FALSE\n",
    "    fsfit = fs(X_fake[subset,], Z[subset], maxsteps=1)\n",
    "    split.lm = lm(Z ~ X_fake[,fsinf$vars[1]], subset=subset_c)\n",
    "    splitP_A = c(splitP_A, summary(split.lm)$coef[2,4])\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plot(ecdf(exactP_A), lwd=3, col='blue', xlim=c(0,1), main='Power comparison (all pvalues)')\n",
    "plot(ecdf(splitP_A), lwd=3, col='red', add=TRUE)\n",
    "abline(0,1, lwd=2, lty=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plot(ecdf(exactP_A[correct_model]), lwd=3, col='blue', xlim=c(0,1), \n",
    "     main='Power comparison (correct model)')\n",
    "plot(ecdf(splitP_A[correct_model]), lwd=3, col='red', add=TRUE)\n",
    "abline(0,1, lwd=2, lty=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Confidence intervals\n",
    "\n",
    "- Which has shorter confidence intervals?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "exactL = c()\n",
    "splitL = c()\n",
    "correct_model = c()\n",
    "\n",
    "beta1 = 0.3\n",
    "for (i in 1:nsim) {\n",
    "    Z = rnorm(nrow(X_fake)) + X_fake[,1] * beta1 \n",
    "    fsfit = fs(X_fake, Z, maxsteps=1)\n",
    "    fsinf = fsInf(fsfit, sigma=1)\n",
    "    exactL = c(exactL, fsinf$ci[1,2] - fsinf$ci[1,1])\n",
    "\n",
    "    correct_model = c(correct_model, fsinf$vars[1] == 1)\n",
    "    \n",
    "    # data splitting pvalue\n",
    "    \n",
    "    subset = sample(1:nrow(X_fake), nrow(X_fake)/2, replace=FALSE)\n",
    "    subset_c = rep(TRUE, nrow(X)) #_c for confirmatory\n",
    "    subset_c[subset] = FALSE\n",
    "    fsfit = fs(X_fake[subset,], Z[subset], maxsteps=1)\n",
    "    split.lm = lm(Z ~ X_fake[,fsinf$vars[1]], subset=subset_c)\n",
    "    conf = confint(split.lm, level=0.9)\n",
    "    splitL = c(splitL, conf[2,2] - conf[2,1])\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "data.frame(median(exactL), median(splitL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "data.frame(median(exactL[correct_model]), median(splitL[correct_model]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### An exciting area of research\n",
    "\n",
    "This `exactP` was only discovered [recently](http://amstat.tandfonline.com/doi/abs/10.1080/01621459.2015.1108848)!\n",
    "\n",
    "For a long time, I have been saying things along the lines of\n",
    "\n",
    "    Inference after model selection \n",
    "    is basically out the window. \n",
    "    \n",
    "    Forget all we taught you about t and \n",
    "    F distributions as it is no longer true...\n",
    "    \n",
    "    Use data splitting!\n",
    "    \n",
    "It turns out that inference after selection is possible, and it\n",
    "doesn't force us to throw away all of our tools for inference.\n",
    "\n",
    "*But, it is a little more complicated to describe.*\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
