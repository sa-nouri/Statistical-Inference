{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Correlated Errors\n",
    "\n",
    "Today, we will consider another departure from our usual model for the variance\n",
    "(i.e. equal variance $\\sigma^2$ and independent).\n",
    "\n",
    "Before we do this, let's refresh our memory on *weighted least squares.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "options(repr.plot.width=5, repr.plot.height=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Weighted least squares\n",
    "\n",
    "In the last set of notes, we considered a model\n",
    "$$\n",
    "Y = X\\beta + \\epsilon, \\qquad \\epsilon \\sim N(0, W^{-1})\n",
    "$$\n",
    "where\n",
    "$$\n",
    "W^{-1} = \\sigma^2 \\cdot \\text{diag}(V_1, \\dots, V_n).\n",
    "$$\n",
    "\n",
    "This model has independent errors, but of different variance: a *heteroscedastic* model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The fix\n",
    "\n",
    "We saw that by defining\n",
    "$$\n",
    "\\tilde{Y} = W^{1/2}Y, \\qquad \\tilde{X} = W^{1/2}X\n",
    "$$\n",
    "we transformed our original model to more familiar model:\n",
    "$$\n",
    "\\tilde{Y} = \\tilde{X}\\beta + \\varepsilon, \\qquad \\varepsilon \\sim N(0, \\sigma^2 I).\n",
    "$$\n",
    "\n",
    "The usual estimator in this model is the *WLS* estimator\n",
    "$$\n",
    "\\hat{\\beta}_W = (X^TWX)^{-1}X^TWY.\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The implications\n",
    "\n",
    "If we ignore *heteroscedasticity* then our *OLS* estimator has\n",
    "distribution\n",
    "$$\n",
    "\\hat{\\beta} = (X^TX)^{-1}X^TY \\sim N(\\beta, \\sigma^2 (X^TX)^{-1} X^TW^{-1}X (X^TX)^{-1}).\n",
    "$$\n",
    "\n",
    "This form of the variance matrix is called the *sandwich form*. \n",
    "\n",
    "**This means that our `Std. Error` column will be off! In other words, `R` will report $t$ statistics that are off by some multiplicative factor!**\n",
    "\n",
    "Another reason to worry about $W$ is that if we use the correct $W$, we have\n",
    "a more *efficient* unbiased estimator: smaller confidence intervals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The implications\n",
    "\n",
    "Using the correct $W$ *proportional to inverse variance of the errors* and\n",
    "form the WLS estimator we have\n",
    "$$\n",
    "\\hat{\\beta}_{WLS} \\sim N(\\beta, \\sigma^2 (X^TWX)^{-1}).\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Autocorrelation\n",
    "\n",
    "- The model of the variance that we will consider today \n",
    "is a model where the errors are *correlated*.\n",
    "\n",
    "- In the random effects model, outcomes within groups were correlated.\n",
    "Other regression applications also have correlated outcomes (i.e.\n",
    "errors).\n",
    "\n",
    "- Common examples of this type of errors occur in time series data, a\n",
    "common model for financial applications.\n",
    "\n",
    "### Why should we worry?\n",
    "Just as in the *heteroscedastic case*, ignoring autocorrelation can lead to underestimates of `Std. Error` $\\rightarrow$ inflated $t$’s $\\rightarrow$ false positives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What is autocorrelation?\n",
    "\n",
    "* Suppose we plot Palo Alto’s daily average temperature – clearly we\n",
    "  would see a pattern in the data.\n",
    "\n",
    "* Sometimes, this pattern can be attributed to a deterministic\n",
    "  phenomenon (i.e. predictable seasonal fluctuations).\n",
    "\n",
    "* Other times, “patterns” are due to correlations in the noise, maybe\n",
    "  small time fluctuations in the stock market, economy, etc.\n",
    "\n",
    "* Example: financial time series: NASDAQ close prices.\n",
    "\n",
    "* Example: residuals regressing consumer expenditure on money stock (this one is discussed in your textbook and used as an example below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Average Max Temp in Palo Alto\n",
    "\n",
    "The daily max temperature shows clear seasonal fluctuations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "PA.temp <- read.table('http://stats191.stanford.edu/data/paloaltoT.table', header=F, skip=2)\n",
    "plot(PA.temp[,3], xlab='Day', ylab='Average Max Temp (F)', pch=23, bg='orange')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### NASDAQ daily close 2011\n",
    "\n",
    "- Another example of a time series can be found from financial data. \n",
    "The price of many assets fluctuate from day to day. \n",
    "\n",
    "- Still, there is a *pattern* in this process. \n",
    "\n",
    "- Given enough information, we might try to also explain\n",
    "this pattern as a deterministic model, like the temperature data. (This is, in some sense, what business news sites try to do on a daily basis).\n",
    "\n",
    "- A simpler model for this pattern is that of some unexplainable noise...\n",
    "\n",
    "- Below, we plot some\n",
    "closing prices of NASDAQ for the year 2011. Data was obtained from\n",
    "on [yahoo finance](http://finance.yahoo.com).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "fname = 'http://stats191.stanford.edu/data/nasdaq_2011.csv'\n",
    "nasdaq.data = read.table(fname, header=TRUE, sep=',')\n",
    "plot(nasdaq.data$Date, nasdaq.data$Close, xlab='Date', ylab='NASDAQ close')\n",
    "abline(h=mean(nasdaq.data$Close))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "ndays = length(nasdaq.data$Date)\n",
    "log_return = log(nasdaq.data$Close[2:ndays] / nasdaq.data$Close[1:(ndays-1)])\n",
    "plot(nasdaq.data$Date[2:ndays], \n",
    "    log_return, xlab='Date', ylab='log(NASDAQ return)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### NASDAQ daily close 2011, ACF\n",
    "\n",
    "- One way this noise is measured is through the *ACF (Auto-Correlation Function)*, which we will define below.\n",
    "\n",
    "- A time series with no auto-correlation (i.e. our usual multiple linear regression model) has an ACF that contains only a spike at 0.\n",
    "\n",
    "- The NASDAQ close clearly has some auto-correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "acf(nasdaq.data$Close)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "acf(log_return)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ACF of independent noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "acf(rnorm(length(nasdaq.data$Close)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Expenditure vs. stock\n",
    "\n",
    "The example we will consider is that of *consumer expenditure* vs. *money stock*, the supply of available money in the economy.\n",
    "\n",
    "Data is collected yearly, so perhaps there is autocorrelation in the model\n",
    "$$\n",
    "{\\tt Expenditure}_t = \\beta_0 + \\beta_1 {\\tt Stock}_t + \\epsilon_t\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "fname = 'http://stats191.stanford.edu/data/expenditure.table'\n",
    "expenditure.table =read.table(fname, header=T)\n",
    "attach(expenditure.table)\n",
    "plot(Stock, Expenditure, pch=23, bg='orange', cex=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Expenditure vs. stock: residuals\n",
    "\n",
    "A plot of residuals against `time`, i.e. their index may show \n",
    "evidence of autocorrelation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "exp.lm = lm(Expenditure ~ Stock)\n",
    "plot(resid(exp.lm), type='l', lwd=2, col='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### ACF of residuals\n",
    "\n",
    "A plot of the ACF may also help. Since there seem to be some\n",
    "points outside the confidence bands, this is some evidence that auto-correlation\n",
    "is present in the errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "acf(resid(exp.lm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Models for correlated errors\n",
    "\n",
    "\n",
    "\n",
    "### AR(1) noise\n",
    "\n",
    "* Suppose that, instead of being independent, the errors in our model\n",
    "  were $$\\varepsilon_t = \\rho \\cdot \\varepsilon_{t-1} + \\omega_t, \\qquad -1 <\n",
    "  \\rho < 1$$ with $\\omega_t \\sim N(0,\\sigma^2)$ independent.\n",
    "  \n",
    "* If $\\rho$ is close to 1, then errors are very correlated, $\\rho=0$\n",
    "  is independence.\n",
    "  \n",
    "* This is “Auto-Regressive Order (1)” noise (AR(1)). Many other models\n",
    "  of correlation exist: ARMA, ARIMA, ARCH, GARCH, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### AR(1) noise, $\\rho=0.9$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "nsample = 200\n",
    "rho = 0.95\n",
    "mu = 1.0\n",
    "plot(arima.sim(list(ar=rho), nsample), lwd=2, col='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Autocorrelation function\n",
    "\n",
    "* For a “stationary” time series $(Z_t)_{1 \\leq t \\leq \\infty}$ define\n",
    "  $$ACF(t) = \\text{ Cor}(Z_s, Z_{s+t}).$$\n",
    "* Stationary means that correlation above does not depend on $s$.\n",
    "* For AR(1) model, $$ACF(t) = \\rho^t.$$\n",
    "* For a sample $(Z_1, \\dots, Z_n)$ from a stationary time series\n",
    "  $$\\widehat{ACF}(t) = \\frac{\\sum_{j=1}^{n-t} (Z_j - \\overline{Z})(Z_{t+j} - \\overline{Z})}{\\sum_{j=1}^n(Z_j - \\overline{Z})^2}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### ACF of AR(1) noise, $\\rho=0.9$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "acf(arima.sim(list(ar=0.9), 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Effects on inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* So far, we have just mentioned that things *may* be correlated, but\n",
    "  not thought about how it affects inference.\n",
    "* Suppose we are in the “one sample problem” setting and we observe\n",
    "  $$W_i  = Z_i + \\mu, \\qquad 1 \\leq i \\leq n$$ with the $Z_i$’s from\n",
    "  an $AR(1)$ time series.\n",
    "* It is easy to see that $$E(\\overline{W}) = \\mu$$ *BUT*, generally \n",
    "  $$\\text{Var}(\\overline{W}) >  \\frac{\\text{Var}(Z_1)}{n}$$ how much\n",
    "  bigger depends on $\\rho.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Misleading inference ignoring autocorrelation\n",
    "\n",
    "Just as in weighted least squares, ignoring the autocorrelation\n",
    "yields misleading `Std. Error` values.\n",
    "\n",
    "Below, we show that ignoring autocorrelation will yield\n",
    "incorrect confidence intervals. The red curve is (an estimate of) the true \n",
    "density of the sample mean, while the blue curve is what\n",
    "we think it should be if the errors were independent. The blue\n",
    "curve is way too optimistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "ntrial = 1000\n",
    "\n",
    "sample.mean = numeric(ntrial)\n",
    "sample.var = numeric(ntrial)\n",
    "\n",
    "for (i in 1:ntrial) {\n",
    "  cur.sample = arima.sim(list(ar=rho), nsample) + mu\n",
    "  sample.mean[i] = mean(cur.sample)\n",
    "  sample.var[i] = var(cur.sample)\n",
    "}\n",
    "\n",
    "data.frame(mean=mean(sample.mean), sd=sqrt(mean(sample.var)))\n",
    "\n",
    "xval = seq(-5,5,0.05)\n",
    "Y = c(density(sample.mean)$y, dnorm(xval, mean=mean(sample.mean),\n",
    "                  sd=sqrt(mean(sample.var) / nsample)))\n",
    "X = c(density(sample.mean)$x, xval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "plot(X, Y, type='n', main='Actual and \"naive\" density of sample mean', xlim=c(-1,3))\n",
    "lines(xval, dnorm(xval, mean=mean(sample.mean),\n",
    "                  sd=sqrt(mean(sample.var) / nsample)), lwd=4, col='blue')\n",
    "lines(density(sample.mean), lwd=4, col='red')\n",
    "legend(-1,1, c('actual', 'naive'), col=c('red', 'blue'), lwd=rep(4,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Regression model with auto-correlated errors (AR(1))\n",
    "\n",
    "* Observations:\n",
    "  $$Y_t = \\beta_0 + \\sum_{j=1}^p X_{tj} \\beta_j + \\varepsilon_t, \\qquad 1 \\leq t \\leq n$$\n",
    "* Errors:\n",
    "  $$\\varepsilon_t = \\rho \\cdot \\varepsilon_{t-1} + \\omega_t, \\qquad -1 < \\rho < 1$$\n",
    "* Question: how do we determine if autocorrelation is present?\n",
    "* Question: what do we do to correct for autocorrelation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Graphical checks for autocorrelation\n",
    "\n",
    "* A plot of residuals vs. time is helpful.\n",
    "* Residuals clustered above and below 0 line can indicate\n",
    "  autocorrelation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Expenditure vs. stock: residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "exp.lm = lm(Expenditure ~ Stock)\n",
    "plot(resid(exp.lm), type='l', lwd=2, col='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Durbin-Watson test\n",
    "\n",
    "* In regression setting, if noise is AR(1), a simple estimate of\n",
    "  $\\rho$ is obtained by (essentially) regressing $e_t$ onto $e_{t-1}$\n",
    "  $$\\widehat{\\rho} = \\frac{\\sum_{t=2}^n \\left(e_t e_{t-1}\\right)}{\\sum_{t=1}^n e_t^2}.$$\n",
    "* To formally test $H_0:\\rho=0$ (i.e. whether residuals are\n",
    "  independent vs. they are AR(1)), use Durbin-Watson test, based on\n",
    "  $$d \\approx 2(1 - \\widehat{\\rho}).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Correcting for AR(1)\n",
    "\n",
    "* Suppose we know $\\rho$, we can then “whiten” the data and regressors\n",
    "  $$\\begin{aligned}\n",
    "     \\tilde{Y}_{t+1} &= Y_{t+1} - \\rho Y_t, t > 1   \\\\\n",
    "     \\tilde{X}_{(t+1),j} &= X_{(t+1),j} - \\rho X_{t,j}, i > 1\n",
    "     \\end{aligned}$$ for $1 \\leq t \\leq n-1$. This model satisfies\n",
    "  “usual” assumptions, i.e. the errors\n",
    "  $$\\tilde{\\varepsilon}_t = \\omega_{t+1} = \\varepsilon_{t+1} - \\rho \\cdot \\varepsilon_t$$\n",
    "  are independent $N(0,\\sigma^2)$.\n",
    "* For coefficients in new model $\\tilde{\\beta}$,\n",
    "  $\\beta_0 = \\tilde{\\beta}_0 / (1 - \\rho)$,\n",
    "  $\\beta_j = \\tilde{\\beta}_j.$\n",
    "* Problem: in general, we don’t know $\\rho$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Two-stage regression\n",
    "\n",
    "As in weighted least squares, we will use a two-stage procedure.\n",
    "\n",
    "* Step 1: Fit linear model to unwhitened data (OLS: ordinary least\n",
    "  squares, i.e. no prewhitening).\n",
    "* Step 2: Estimate $\\rho$ with $\\widehat{\\rho}$.\n",
    "* Step 3: Pre-whiten data using $\\widehat{\\rho}$ – refit the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Whitening\n",
    "\n",
    "Our solution in the weighted least squares and auto-correlated errors\n",
    "examples were the same. This procedure is generally called *whitening*.\n",
    "\n",
    "Consider a model\n",
    "$$\n",
    "Y = X\\beta + \\epsilon, \\qquad \\epsilon \\sim N(0, \\Sigma).\n",
    "$$\n",
    "\n",
    "If $\\Sigma$ is invertible, then we can find a inverse square root of $\\Sigma$:\n",
    "$$\n",
    "\\Sigma^{-1/2}\\Sigma (\\Sigma^{-1/2})^T = I, \\qquad (\\Sigma^{-1/2})^T\\Sigma^{-1/2} = \\Sigma^{-1}.\n",
    "$$\n",
    "\n",
    "Define\n",
    "$$\n",
    "\\tilde{Y} = \\Sigma^{-1/2}Y, \\qquad \\tilde{X} = \\Sigma^{-1/2}X.\n",
    "$$\n",
    "Then\n",
    "$$\n",
    "\\tilde{Y} = \\tilde{X}\\beta + \\tilde{\\epsilon}, \\qquad \\tilde{\\epsilon} \\sim N(0, I).\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Generalized least squares\n",
    "\n",
    "- The OLS estimator based on $(\\tilde{Y}, \\tilde{X})$ is \n",
    "$$\n",
    "\\hat{\\beta}_{\\Sigma} = (X^T\\Sigma^{-1}X)^{-1}X^T\\Sigma^{-1}Y \\sim N(\\beta, (X^T\\Sigma^{-1}X)^{-1})\n",
    "$$\n",
    "\n",
    "- It is often called the *GLS (Generalized Least Squares)* estimate based on\n",
    "the covariance matrix $\\Sigma$.\n",
    "\n",
    "- The OLS estimator based on $(Y,X)$ has the sandwich form again:\n",
    "$$\n",
    "\\hat{\\beta} = (X^TX)^{-1}X^TY \\sim N(\\beta, (X^TX)^{-1}X^T\\Sigma X (X^TX)^{-1}).\n",
    "$$\n",
    "\n",
    "- As in WLS, the GLS estimator with $\\Sigma=\\text{Var}(Y)$ will generally be a more efficient estimator.\n",
    "\n",
    "- WLS is special case when $\\Sigma$ is diagonal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Interpreting results of two-stage fit\n",
    "\n",
    "* Basically, interpretation is unchanged, but the exact degrees of\n",
    "  freedom in the error is not exactly clear.\n",
    "\n",
    "* Commonly applied argument: \n",
    "  “this works for large degrees of freedom, so we\n",
    "  hope we have enough degrees of freedom so this point is not\n",
    "  important.”\n",
    "\n",
    "* Can treat $t$-statistics as $Z$-statistics, $F$’s as $\\chi^2$,\n",
    "  appealing to asymptotics:\n",
    "\n",
    "  * $t_{\\nu}$, with $\\nu$ large is like $N(0,1)$;\n",
    "  * $F_{j,\\nu}$, with $\\nu$ large is like $\\chi^2_j/j.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Expenditure vs. stock: Durbin-Watson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "library(car) # durbin.watson is in the \"car\" package\n",
    "durbinWatsonTest(exp.lm)\n",
    "rho.hat = durbinWatsonTest(exp.lm)$r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Given the value of $\\rho$, we can apply our whitening procedure.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "wExp = numeric(length(Expenditure) - 1)\n",
    "wStock = numeric(length(Expenditure) - 1)\n",
    "for (i in 2:length(Expenditure)) {\n",
    "  wExp[i-1] = Expenditure[i] - rho.hat * Expenditure[i-1]\n",
    "  wStock[i-1] = Stock[i] - rho.hat * Stock[i-1]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "After whitening, we refit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "exp.whitened.lm = lm(wExp ~ wStock)\n",
    "plot(resid(exp.whitened.lm), type='l', lwd=2, col='red')\n",
    "#summary(exp.whitened.lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Comparing to our original fit, we see that our $t$ statistic has changed\n",
    "by a factor of roughly 2.5 from 20 to 8.6!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "summary(exp.lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "summary(exp.whitened.lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Lastly, let's take a look at the residuals of the whitened data. If\n",
    "our whitening has been successful, this should just be a spike at 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "acf(resid(exp.whitened.lm))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
