{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Logistic regression\n",
    "\n",
    "Binary outcomes\n",
    "\n",
    "* Most models so far have had response $Y$ as continuous.\n",
    "* Many responses in practice fall into the $YES/NO$ framework.\n",
    "* Examples:\n",
    "   1. medical: presence or absence of cancer\n",
    "   2. financial: bankrupt or solvent\n",
    "   3. industrial: passes a quality control test or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "options(repr.plot.width=5, repr.plot.height=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Modelling probabilities\n",
    "\n",
    "* For $0-1$ responses we need to model \n",
    "$$\\pi(x_1, \\dots, x_p) = P(Y=1|X_1=x_1,\\dots, X_p=x_p)$$\n",
    "* That is, $Y$ is Bernoulli with a probability that depends on covariates $\\{X_1, \\dots, X_p\\}.$\n",
    "* **Note:**\n",
    "   $\\text{Var}(Y) = \\pi ( 1 - \\pi) = E(Y) \\cdot ( 1-  E(Y))$\n",
    "* **Or,**\n",
    "   the binary nature forces a relation between mean and variance of $Y$.\n",
    "* This makes logistic regression a `Generalized Linear Model`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Flu shot example\n",
    "\n",
    "* A local health clinic sent fliers to its clients to encourage everyone, but especially older persons at high risk of complications, to get a flu shot in time for protection against an expected flu epidemic.\n",
    "* In a pilot follow-up study, 50 clients were randomly selected and asked whether they actually received a flu shot. $Y={\\tt Shot}$\n",
    "* In addition, data were collected on their age $X_1={\\tt Age}$ and their health awareness $X_2={\\tt Health.Aware}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### A possible model\n",
    "\n",
    "- Simplest model $\\pi(X_1,X_2) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2$\n",
    "- Problems / issues:\n",
    "     - We must have $0 \\leq E(Y) = \\pi(X_1,X_2) \\leq 1$. OLS will not force this.\n",
    "     - Ordinary least squares will not work because of relation between mean and variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Logistic model\n",
    "\n",
    "* Logistic model $\\pi(X_1,X_2) = \\frac{\\exp(\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2)}{1 + \\exp(\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2)}$\n",
    "* This automatically fixes $0 \\leq E(Y) = \\pi(X_1,X_2) \\leq 1$.\n",
    "* **Define:**\n",
    "   $\\text{logit}(\\pi(X_1, X_2)) = \\log\\left(\\frac{\\pi(X_1, X_2)}{1 - \\pi(X_1,X_2)}\\right) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Logistic distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "logit.inv = function(x) {\n",
    "  return(exp(x) / (1 + exp(x)))\n",
    "}\n",
    "x = seq(-4, 4, length=200)\n",
    "plot(x, logit.inv(x), lwd=2, type='l', col='red', cex.lab=1.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Logistic transform: `logit`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "logit = function(p) {\n",
    "  return(log(p / (1 - p)))\n",
    "}\n",
    "p = seq(0.01,0.99,length=200)\n",
    "plot(p, logit(p), lwd=2, type='l', col='red', cex.lab=1.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Binary regression models\n",
    "\n",
    "* Models $E(Y)$ as  $F(\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2)$ for some increasing function\n",
    "$F$ (usually a distribution function).\n",
    "* The logistic model uses the function (we called `logit.inv` above) $$F(x)=\\frac{e^x}{1+e^x}.$$\n",
    "* Can be fit using Maximum Likelihood / Iteratively Reweighted Least Squares.\n",
    "* For logistic regression, coefficients have nice interpretation in terms of `odds ratios` (to be defined shortly).\n",
    "  \n",
    "* What about inference?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Criterion used to fit model\n",
    "\n",
    "Instead of sum of squares, logistic regression \n",
    "uses *deviance*:\n",
    "\n",
    "* $DEV(\\mu| Y) = -2 \\log L(\\mu| Y) + 2 \\log L(Y| Y)$ where $\\mu$ is a location estimator for $Y$.\n",
    "* If $Y$ is Gaussian with independent $N(\\mu_i,\\sigma^2)$ entries then $DEV(\\mu| Y) = \\frac{1}{\\sigma^2}\\sum_{i=1}^n(Y_i - \\mu_i)^2$\n",
    "* If $Y$ is a binary vector, with mean vector $\\pi$ then \n",
    "$DEV(\\pi| Y) = -2 \\sum_{i=1}^n \\left( Y_i \\log(\\pi_i) + (1-Y_i) \\log(1-\\pi_i) \\right)$\n",
    "\n",
    "**Minimizing deviance $\\iff$ Maximum Likelihood**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Deviance for logistic regression\n",
    "\n",
    "* For any binary regression model, $\\pi=\\pi(\\beta)$.\n",
    "\n",
    "* The deviance is:\n",
    "$$\\begin{aligned}\n",
    "     DEV(\\beta| Y) &=  -2 \\sum_{i=1}^n \\left( Y_i {\\text{logit}}(\\pi_i(\\beta)) + \\log(1-\\pi_i(\\beta)) \\right)\n",
    "     \\end{aligned}$$\n",
    "     \n",
    "* For the logistic model, the RHS is:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    -2 \\left[ (X\\beta)^Ty + \\sum_{i=1}^n\\log \\left(1 + \\exp \\left(\\sum_{j=1}^p X_{ij} \\beta_j\\right) \\right)\\right]\n",
    "     \\end{aligned}$$\n",
    "   \n",
    "* The logistic model is special in that $\\text{logit}(\\pi(\\beta))=X\\beta$. If we used\n",
    "a different transformation, the first part would not be linear in $X\\beta$.\n",
    "\n",
    "* *For ease of notation I'm assuming that `X[,1]=1` corresponding to $\\beta_0$*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "flu.table = read.table('http://stats191.stanford.edu/data/flu.table', \n",
    "                       header=TRUE)\n",
    "flu.glm = glm(Shot ~ Age + Health.Aware, data=flu.table, \n",
    "              family=binomial())\n",
    "summary(flu.glm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Odds Ratios\n",
    "\n",
    "* One reason logistic models are popular is that the parameters have simple interpretations in terms of **odds**\n",
    "   $$ODDS(A) = \\frac{P(A)}{1-P(A)}.$$\n",
    "* Logistic model: $$OR_{X_j} = \\frac{ODDS(Y=1|\\dots, X_j=x_j+h, \\dots)}{ODDS(Y=1|\\dots, X_j=x_j, \\dots)} = e^{h \\beta_j}$$\n",
    "* If $X_j \\in {0, 1}$ is dichotomous, then odds for group with $X_j = 1$ are $e^{\\beta_j}$ higher, other parameters being equal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###  Rare disease hypothesis\n",
    "\n",
    "* When incidence is rare, $P(Y=0)\\approxeq 1$ no matter what the covariates $X_j$â€™s are.\n",
    "* In this case, odds ratios are almost ratios of probabilities: $$OR_{X_j} \\approxeq \\frac{{\\mathbb{P}}(Y=1|\\dots, X_j=x_j+1, \\dots)}{{\\mathbb{P}}(Y=1|\\dots, X_j=x_j, \\dots)}$$\n",
    "* Hypothetical example: in a lung cancer study, if $X_j$ is an indicator of smoking or not, a $\\beta_j$ of 5 means for smoking vs. non-smoking means smokers are $e^5 \\approx 150$ times more likely to develop lung cancer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###  Rare disease hypothesis\n",
    "\n",
    "* In flu example, the odds ratio for a 45 year old with health awareness 50 compared to a 35 year old with the same health awareness are\n",
    "$$e^{-1.429284+3.647052}=9.18$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "logodds = predict(flu.glm, list(Age=c(35,45),Health.Aware=c(50,50)),\n",
    "                 type='link')\n",
    "logodds\n",
    "exp(logodds[2] - logodds[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The estimated probabilities are below, yielding a ratio of $0.1932/0.0254 \\approx 7.61$. Not too far from 9.18."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "prob = exp(logodds)/(1+exp(logodds))\n",
    "prob\n",
    "prob[2] / prob[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Iteratively reweighted least squares\n",
    "\n",
    "### An algorithm to fit the model\n",
    "\n",
    "1. Initialize $\\widehat{\\pi}_i = \\bar{Y}, 1 \\leq i \\leq n$\n",
    "2. Define $$Z_i = g(\\widehat{\\pi}_i) + g'(\\widehat{\\pi}_i) (Y_i - \\widehat{\\pi_i})$$\n",
    "3. Fit weighted least squares model \n",
    "$$Z_i \\sim \\sum_{j=1}^p \\beta_j X_{ij}, \\qquad w_i = \\widehat{\\pi_i} (1 - \\widehat{\\pi}_i)$$\n",
    "4. Set $\\widehat{\\pi}_i = \\text{logit}^{-1} \\left(\\widehat{\\beta}_0 + \\sum_{j=1}^p \\widehat{\\beta}_j X_{ij}\\right)$.\n",
    "5. Repeat steps 2-4 until convergence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Newton-Raphson\n",
    "\n",
    "The Newton-Raphson updates for logistic regression are\n",
    "$$\n",
    "\\hat{\\beta} \\mapsto \\hat{\\beta} - \\nabla^2 DEV(\\hat{\\beta})^{-1} \\nabla DEV(\\hat{\\beta})\n",
    "$$\n",
    "\n",
    "- These turn out to be the same as the updates above.\n",
    "\n",
    "- In earlier statistical software one might only have access to a weighted least squares estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Inference\n",
    "\n",
    "One thing the IRLS procedure hints at is what the approximate\n",
    "limiting distribution is.\n",
    "\n",
    "* The IRLS procedure suggests using approximation $\\widehat{\\beta} \\approx N(\\beta, (X^TWX)^{-1})$\n",
    "* This allows us to construct CIs, test linear hypotheses, etc.\n",
    "* Intervals formed this way are called *Wald intervals*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "center = coef(flu.glm)['Age']\n",
    "SE = sqrt(vcov(flu.glm)['Age', 'Age'])\n",
    "U = center + SE * qnorm(0.975)\n",
    "L = center - SE * qnorm(0.975)\n",
    "data.frame(L, center, U)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Covariance\n",
    "\n",
    "- The estimated covariance uses the weights computed from the fitted\n",
    "model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pi.hat = fitted(flu.glm)\n",
    "W.hat = pi.hat * (1 - pi.hat)\n",
    "X = model.matrix(flu.glm)\n",
    "C = solve(t(X) %*% (W.hat * X))\n",
    "c(SE, sqrt(C['Age', 'Age']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Confidence intervals\n",
    "\n",
    "- The intervals above are slightly different from what `R` will give you if you ask it for \n",
    "confidence intervals.\n",
    "\n",
    "- `R` uses so-called profile intervals. \n",
    "\n",
    "- For large samples the two methods should agree quite closely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "CI = confint(flu.glm)\n",
    "CI\n",
    "mean(CI[2,]) # profile intervals are not symmetric around the estimate...\n",
    "data.frame(L, center, U)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Testing in logistic regression\n",
    "\n",
    "What about comparing full and reduced model?\n",
    "\n",
    "- For a model ${\\cal M}$, $DEV({\\cal M})$ replaces $SSE({\\cal M})$.\n",
    "- In least squares regression (with $\\sigma^2$ known), we use \n",
    "$$\\frac{1}{\\sigma^2}\\left( SSE({\\cal M}_R) - SSE({\\cal M}_F) \\right) \\overset{H_0:{\\cal M}_R}{\\sim} \\chi^2_{df_R-df_F}$$\n",
    "- This is closely related to $F$ with large $df_F$: approximately $F_{df_R-df_F, df_R} \\cdot (df_R-df_F)$.\n",
    "\n",
    "- For logistic regression this difference in $SSE$ is replaced with \n",
    "$$DEV({\\cal M}_R) - DEV({\\cal M}_F) \\overset{n \\rightarrow \\infty, H_0:{\\cal M}_R}{\\sim} \\chi^2_{df_R-df_F}$$\n",
    "- Resulting tests do not agree numerically with those coming from IRLS (Wald tests). Both are often used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "anova(glm(Shot ~ 1,\n",
    "          data=flu.table, \n",
    "          family=binomial()), \n",
    "      flu.glm)\n",
    "anova(glm(Shot ~ Health.Aware,\n",
    "          data=flu.table, \n",
    "          family=binomial()), \n",
    "      flu.glm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We should compare this difference in deviance with a $\\chi^2_1$ random variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "1 - pchisq(35.61, 2) # testing ~1 vs ~1 + Health.Aware + Age\n",
    "1 - pchisq(16.863, 1) # testing ~ 1 + Health.Aware vs ~1 + Health.Aware + Age"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's compare this with the Wald test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "summary(flu.glm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Diagnostics\n",
    "\n",
    "* Similar to least square regression, only residuals used are usually *deviance residuals*\n",
    "   $r_i = \\text{sign}(Y_i-\\widehat{\\pi}_i) \\sqrt{DEV(\\widehat{\\pi}_i|Y_i)}.$\n",
    "   \n",
    "* These agree with usual residual for least square regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "par(mfrow=c(2,2))\n",
    "plot(flu.glm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "influence.measures(flu.glm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Model selection\n",
    "\n",
    "As the model is a likelihood based model, each fitted model has an AIC.\n",
    "Stepwise selection can be used easily â€¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "step(flu.glm, scope=list(upper= ~.^2), direction='both')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Penalized regression\n",
    "\n",
    "### LASSO\n",
    "\n",
    "- Instead of just minimizing deviance, we can also look at penalized versions\n",
    "$$\n",
    "\\text{minimize}_{\\beta} \\frac{1}{2n} DEV(\\beta) + \\lambda \\|\\beta\\|_1\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "library(glmnet)\n",
    "X = model.matrix(flu.glm)[,-1]\n",
    "Y = as.numeric(flu.table$Shot)\n",
    "G = glmnet(X, Y, family=\"binomial\")\n",
    "plot(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "library(ElemStatLearn)\n",
    "data(spam)\n",
    "dim(spam)\n",
    "X = model.matrix(spam ~ ., data=spam)[,-1]\n",
    "Y = as.numeric(spam$spam == 'spam')\n",
    "G = glmnet(X, Y, family='binomial')\n",
    "plot(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "CV = cv.glmnet(X, Y, family='binomial')\n",
    "plot(CV)\n",
    "c(CV$lambda.min, CV$lambda.1se)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Extracting coefficients from `glmnet`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "beta.hat = coef(G, s=CV$lambda.1se)\n",
    "beta.hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Probit model\n",
    "\n",
    "* Probit regression model: $$\\Phi^{-1}(E(Y|X))= \\sum_{j=1}^{p} \\beta_j X_{j}$$ where $\\Phi$ is CDF of $N(0,1)$, i.e. $\\Phi(t) = {\\tt pnorm(t)}$, $\\Phi^{-1}(q) = {\\tt qnorm}(q)$.\n",
    "\n",
    "* Regression function\n",
    "$$\n",
    "\\begin{aligned}\n",
    "E(Y|X) &= E(Y|X_1,\\dots,X_p) \\\\\n",
    "&= P(Y=1|X_1, \\dots, X_p) \\\\\n",
    "& = {\\tt pnorm}\\left(\\sum_{j=1}^p \\beta_j X_j \\right)\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "* In logit, probit and cloglog ${\\text{Var}}(Y_i)=\\pi_i(1-\\pi_i)$ but the model for the mean is different.\n",
    "\n",
    "* Coefficients no longer have an odds ratio interpretation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "summary(glm(Shot ~ Age + Health.Aware, \n",
    "            data=flu.table, \n",
    "            family=binomial(link='probit')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Generalized linear models\n",
    "\n",
    "Given a dataset $(Y_i, X_{i1}, \\dots, X_{ip}), 1 \\leq i \\leq n$ we consider a model for the distribution of $Y|X_1, \\dots, X_p$.\n",
    "* If $\\eta_i=g(E(Y_i|X_i)) = g(\\mu_i) =  \\sum_{j=1}^p \\beta_j X_{ij}$ then $g$ is called the *link*\n",
    "   function for the model.\n",
    "* If ${\\text{Var}}(Y_i) = \\phi \\cdot V({\\mathbb{E}}(Y_i)) = \\phi \\cdot V(\\mu_i)$ for $\\phi > 0$ and some function $V$, then $V$ is the called *variance*\n",
    "   function for the model.\n",
    "* Canonical reference [Generalized linear models](http://www.amazon.com/Generalized-Edition-Monographs-Statistics-Probability/dp/0412317605)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Binary regression as GLM\n",
    "\n",
    "* For a logistic model, $g(\\mu)={\\text{logit}}(\\mu), \\qquad V(\\mu)=\\mu(1-\\mu).$\n",
    "* For a probit model, $g(\\mu)=\\Phi^{-1}(\\mu), \\qquad V(\\mu)=\\mu(1-\\mu).$\n",
    "* For a cloglog model, $g(\\mu)=-\\log(-\\log(\\mu)), \\qquad V(\\mu)=\\mu(1-\\mu).$\n",
    "* All of these have *dispersion* $\\phi=1$."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
